{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO SAVE PER EPOCH IN NON UNBALANCED SET\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "from math import sqrt\n",
    "import argparse\n",
    "import os\n",
    "from PedrosNetworkFunctions import *\n",
    "\n",
    "#This trainer runs its evalutations on the evaluation set and balanced set separetely after training\n",
    "def train1(test_coch,testlabels,testsize,Saver_Folder,TensorBoard_Folder_train,TensorBoard_Folder_test,batchsize,Num_Epochs,nets,trainsize,train_coch,trainlabels,COCHLEAGRAM_LENGTH):\n",
    "    \n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=5)\n",
    "\n",
    "    sv = tf.train.Supervisor(logdir=\"./Saved_Sessions{0}\".format(Saver_Folder),saver = saver, summary_op=None, save_model_secs = 7200,checkpoint_basename='model.ckpt')\n",
    "    #with tf.Session() as sess:\n",
    "    with sv.managed_session() as sess:\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter(TensorBoard_Folder_train,sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(TensorBoard_Folder_test,sess.graph)\n",
    "\n",
    "        #nets['coch_images']\n",
    "        startindex = 0\n",
    "        endindex = startindex+batchsize\n",
    "        '''if(print_extras):\n",
    "            coch_images_sum = sess.run(nets['coch_images'],feed_dict={nets['input_to_net']:test_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH]})\n",
    "            train_writer.add_summary(coch_images_sum, 0)  '''  \n",
    "\n",
    "        #LOOP THROUGH EPOCHS\n",
    "        print(\"starting\")\n",
    "        for epoch in range(sess.run(nets['current_epoch']),Num_Epochs):\n",
    "            if(sv.should_stop()):\n",
    "                break\n",
    "                \n",
    "            image_conv1 = sess.run(nets['conv1_weight_image'])\n",
    "            train_writer.add_summary(image_conv1, epoch)\n",
    "            #print(\"epoch: \",epoch)\n",
    "            #Train a full epoch\n",
    "            startindex = sess.run(nets['current_step']) * batchsize\n",
    "            endindex = startindex+batchsize\n",
    "\n",
    "            while(endindex <= trainsize):\n",
    "                '''if(print_extras):\n",
    "                    _,cross_entropySummary,l1Sums,__ = sess.run([nets['train_step'],nets['cross_entropy_summary'],nets['layer1'],nets['increment_current_step']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['keep_prob']:.5})\n",
    "                    train_writer.add_summary(cross_entropySummary, epoch)\n",
    "                    train_writer.add_summary(l1Sums, epoch)\n",
    "                else:'''\n",
    "\n",
    "                sess.run([nets['train_step'],nets['increment_current_step']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['keep_prob']:.5})\n",
    "\n",
    "                startindex = endindex\n",
    "                endindex = startindex+batchsize\n",
    "\n",
    "      #EVALUATION!!!      \n",
    "            if(epoch%1 == 0):\n",
    "                #print(\"eval\")\n",
    "                #max_to_keep\n",
    "                #keep_checkpoint_every_n_hours\n",
    "                #Evaluate the full dataset\n",
    "                ###\n",
    "                #TEST SET\n",
    "                startindex = 0\n",
    "                endindex = startindex+batchsize\n",
    "                total_count = 0\n",
    "                correct = 0\n",
    "                correct_train = 0\n",
    "                while(endindex <= testsize):\n",
    "                    trainBatchLabels = testlabels[startindex:endindex]\n",
    "                    indeces_list = []\n",
    "                    for row in trainBatchLabels:\n",
    "                        indeces= np.where(row==1)[0]\n",
    "                        total_count = total_count + len(indeces)\n",
    "                        indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "\n",
    "                    '''if(print_extras):\n",
    "                        addme,pred_sum  = sess.run([nets['numCorrect'],prediction_sums],feed_dict={nets['input_to_net']:test_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "                        train_writer.add_summary(pred_sum, epoch + startindex/testsize)\n",
    "                        correct = correct + addme\n",
    "                    else:'''\n",
    "\n",
    "                    addme  = sess.run(nets['numCorrect'],feed_dict={nets['input_to_net']:test_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "                    correct = correct + addme    \n",
    "\n",
    "\n",
    "                    '''if(print_extras):\n",
    "                        if(startindex == 0):\n",
    "                            conv1array,conv1weights,conv1s = sess.run([nets['conv1'],nets['conv1_Weights'],nets['conv1_sums']],feed_dict={nets['input_to_net']:test_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "                            train_writer.add_summary(conv1s)'''\n",
    "\n",
    "\n",
    "                    startindex = endindex\n",
    "                    endindex = startindex+batchsize\n",
    "\n",
    "                #print(\"Epoch {0}, accuracy {1}\".format(epoch,correct/total_count))\n",
    "                summary = sess.run(nets['accsum'],feed_dict={nets['accuracy']:correct/total_count})\n",
    "                test_writer.add_summary(summary, epoch)\n",
    "\n",
    "\n",
    "                #TRAINSET\n",
    "                startindex = 0\n",
    "                endindex = startindex+batchsize\n",
    "                total_count = 0\n",
    "                correct = 0\n",
    "                while(endindex <= trainsize):\n",
    "                    trainBatchLabels = trainlabels[startindex:endindex]\n",
    "                    indeces_list = []\n",
    "                    for row in trainBatchLabels:\n",
    "                        indeces= np.where(row==1)[0]\n",
    "                        total_count = total_count + len(indeces)\n",
    "                        indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "\n",
    "                    addme  = sess.run(nets['numCorrect'],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "                    correct = correct + addme    \n",
    "                    startindex = endindex\n",
    "                    endindex = startindex+batchsize\n",
    "\n",
    "                #print(\"Epoch {0}, accuracy {1}\".format(epoch,correct/total_count))\n",
    "                summary = sess.run(nets['accsum'],feed_dict={nets['accuracy']:correct/total_count})\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            sess.run(nets['increment_current_epoch'])\n",
    "            sess.run(nets['reset_current_step'])\n",
    "            sv.saver.save(sess,'./Saved_Sessions{0}/model.ckpt'.format(Saver_Folder))\n",
    "\n",
    "            '''from tensorflow.python.client import timeline\n",
    "            trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "            trace_file = open('timeline.ctf.json', 'w')\n",
    "            trace_file.write(trace.generate_chrome_trace_format())'''\n",
    "            #endindex = startindex+batchsize\n",
    "            #potato = sess.run(nets['indeces'],feed_dict={nets['input_to_net']:test_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "            #print()\n",
    "            #print(start)\n",
    "            #print(end)\n",
    "            #print(total_count)\n",
    "            #print(correct)\n",
    "            #print(\"actual: \", indeces_list)\n",
    "            #print(\"predicted :\", potato)\n",
    "            #print()\n",
    "            #f= open(\"output3.csv\",\"a\")\n",
    "            #f.write(\"loop {0}, accuracy {1}\\n\".format(i,correct/total_count))\n",
    "                #f.close()''\n",
    "\n",
    "                \n",
    "#runs evaluation on the trainset itself. Splits training set 90 to 10\n",
    "def train2(test_coch,testlabels,testsize,Saver_Folder,TensorBoard_Folder_train,TensorBoard_Folder_test,batchsize,Num_Epochs,nets,trainsize,train_coch,trainlabels,COCHLEAGRAM_LENGTH,cutoff,TASK,SAVE = True):\n",
    "    #This saver saves the graph at every epoch, and never deletes/uses them. (otherwise the superviser will delete old saves)\n",
    "    epochsaver = tf.train.Saver(max_to_keep=0) #saves all the epochs\n",
    "    if(not os.path.exists(\"EPOCH_SAVED{0}/\".format(Saver_Folder))):\n",
    "        try:\n",
    "            os.makedirs(\"EPOCH_SAVED{0}/\".format(Saver_Folder))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if SAVE:\n",
    "        save_model_secs = 7200\n",
    "        keep_checkpoint_every_n_hours = 5\n",
    "    else:\n",
    "        save_model_secs = 0\n",
    "        keep_checkpoint_every_n_hours =0\n",
    "    \n",
    "    #This saver saves every 5 hours but only keeps the latest 5. for backups in case we crash\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n",
    "    sv = tf.train.Supervisor(logdir=\"./Saved_Sessions{0}\".format(Saver_Folder),saver = saver, summary_op=None, save_model_secs = save_model_secs,checkpoint_basename='model.ckpt')\n",
    "    \n",
    "    print(\"initializing\")\n",
    "    with sv.managed_session() as sess: #runs a supervised session\n",
    "        train_writer = tf.summary.FileWriter(TensorBoard_Folder_train,sess.graph) #tensorboard writters for both train and test accuracy\n",
    "        test_writer = tf.summary.FileWriter(TensorBoard_Folder_test,sess.graph)\n",
    "\n",
    "        startindex = 0\n",
    "        endindex = startindex+batchsize\n",
    "\n",
    "        #LOOP THROUGH EPOCHS\n",
    "        #print(\"starting unbalanced\")\n",
    "        print(\"starting\")\n",
    "        for epoch in range(sess.run(nets['current_epoch']),Num_Epochs):\n",
    "            if(sv.should_stop()):\n",
    "                break\n",
    "            print(\"epoch \", epoch)\n",
    "            \n",
    "            if SAVE:\n",
    "                sv.saver.save(sess, 'Saved_Sessions{0}/model.ckpt'.format(Saver_Folder), global_step=epoch)\n",
    "                epochsaver.save(sess, save_path=\"EPOCH_SAVED{0}/model.ckpt\".format(Saver_Folder), global_step=epoch)\n",
    "            \n",
    "            image_conv1 = sess.run(nets['conv1_weight_image'])\n",
    "            train_writer.add_summary(image_conv1,nets['global_step'].eval(session=sess))\n",
    "            #train_writer.add_summary(image_conv1,epoch)\n",
    "            #print(\"epoch: \",epoch)\n",
    "            #Train a full epoch\n",
    "            \n",
    "            startindex = sess.run(nets['current_step']) * batchsize #continue were we left off if we crashed\n",
    "            endindex = startindex+batchsize\n",
    "\n",
    "            #loop through the first 90% of the set\n",
    "            while(endindex <= cutoff):\n",
    "                #Get the indeces as ints for the labels \n",
    "                trainBatchLabels = trainlabels[startindex:endindex]\n",
    "                total_count = 0\n",
    "                correct = 0\n",
    "                correct_train = 0\n",
    "                indeces_list = []\n",
    "                for row in trainBatchLabels:\n",
    "                    indeces= np.where(row==1)[0]\n",
    "                    total_count = total_count + len(indeces)\n",
    "                    indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "\n",
    "                 #run the train step, increment the current step we are in, calculate how many correct predictions           \n",
    "                if TASK == \"PREDICT_LABELS\":\n",
    "                    _,__,correct,XE_SUM = sess.run([nets['train_step'],nets['increment_current_step'],nets['numCorrect'],nets['cross_entropy_summary']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:.5})\n",
    "                else:\n",
    "                    _,__,correct,XE_SUM = sess.run([nets['train_step'],nets['increment_current_step'],nets['numCorrect_top_pred'],nets['cross_entropy_summary']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:.5})\n",
    "                \n",
    "                #write to tensorboard\n",
    "                \n",
    "                summary = sess.run(nets['accsum'],feed_dict={nets['accuracy']:correct/total_count})\n",
    "                #print(\"epoch {0}, {1}\".format(epoch,correct/total_count))\n",
    "                '''print(sess.run([nets['Top_1_index'],nets['actual_labels'],nets['predicted_labels']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:.5}))\n",
    "                print(np.array(indeces_list)[:,0])\n",
    "                print(total_count)\n",
    "                print(correct)\n",
    "                print()'''\n",
    "                train_writer.add_summary(summary,nets['global_step'].eval(session=sess))\n",
    "                train_writer.add_summary(XE_SUM,nets['global_step'].eval(session=sess))\n",
    "                \n",
    "                \n",
    "                #train_writer.add_summary(summary,epoch +startindex/cutoff)\n",
    "                #print(epoch +startindex/cutoff)\n",
    "                startindex = endindex\n",
    "                endindex = startindex+batchsize\n",
    "        \n",
    "                \n",
    "      #EVALUATION\n",
    "        #pretty much the same thing but no train step.\n",
    "            if(epoch%1 == 0):\n",
    "                #print(\"eval\")\n",
    "                #TEST SET\n",
    "                startindex = cutoff #start at the first item in the last 10%\n",
    "                endindex = startindex+batchsize\n",
    "                total_count = 0\n",
    "                correct = 0\n",
    "                correct_train = 0\n",
    "                xe_sum = 0\n",
    "                \n",
    "                numbatches = 0\n",
    "                while(endindex <= trainsize):\n",
    "                    numbatches = numbatches + 1 #TODO: just calculate this\n",
    "                    #print(startindex)\n",
    "                    trainBatchLabels = trainlabels[startindex:endindex]\n",
    "                    indeces_list = []\n",
    "                    for row in trainBatchLabels:\n",
    "                        indeces= np.where(row==1)[0]\n",
    "                        total_count = total_count + len(indeces)\n",
    "                        indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "                    \n",
    "                                        \n",
    "                    if TASK == \"PREDICT_LABELS\":\n",
    "                        addme, xe  = sess.run([nets['numCorrect'],nets['cross_entroy']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "                    else:\n",
    "                        addme, xe   = sess.run([nets['numCorrect_top_pred'],nets['cross_entroy']],feed_dict={nets['input_to_net']:train_coch[startindex:endindex][:,0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainlabels[startindex:endindex],nets['label_indeces']:indeces_list,nets['keep_prob']:1})\n",
    "                   \n",
    "                    correct = correct + addme    \n",
    "                    xe_sum += xe\n",
    "                    startindex = endindex\n",
    "                    endindex = startindex+batchsize\n",
    "\n",
    "                #print(\"Epoch {0}, accuracy {1}\".format(epoch,correct/total_count))\n",
    "                summary = sess.run(nets['accsum'],feed_dict={nets['accuracy']:correct/total_count})\n",
    "                test_writer.add_summary(summary,nets['global_step'].eval(session=sess))\n",
    "                #test_writer.add_summary(summary,epoch)\n",
    "                xesummary = sess.run(nets['xesum'],feed_dict={nets['xe_ave']:xe_sum/numbatches})\n",
    "                test_writer.add_summary(xesummary,nets['global_step'].eval(session=sess))\n",
    "\n",
    "            sess.run(nets['increment_current_epoch'])\n",
    "            sess.run(nets['reset_current_step'])\n",
    "            \n",
    "        if SAVE:\n",
    "            sv.saver.save(sess,'Saved_Sessions{0}/model.ckpt'.format(Saver_Folder), global_step = epoch)             \n",
    "            epochsaver.save(sess,save_path = \"EPOCH_SAVED{0}/model.ckpt\".format(Saver_Folder), global_step = epoch)\n",
    "\n",
    "\n",
    "        \n",
    "def main(Learning_Rate,limit = None,music_only = False, print_extras = True, Net_or_VGG = \"Net\",name=\"\",unbalanced=False, folder = \"TB2\", Conv1_filtersize = 9, TASK = \"PREDICT_LABELS\", OVERIDE_FOLDER = None,SAVE = True, padding = \"SAME\",poolmethod=\"MAXPOOL\",conv1_times_hanning = False,Freeze_Model_File= None):\n",
    "    #if we want to use the unbalanced set for training/eval, read that file in\n",
    "    if(unbalanced):\n",
    "        trainfile = \"/om/user/ribeirop/audiosetDL/unbalanced_stripped/unbalanced_train_segments_Coch.hdf5\"\n",
    "        train_averagefile = \"/om/user/ribeirop/audiosetDL/unbalanced_stripped/unbalanced_train_segments_average.npy\"\n",
    "    else:\n",
    "        trainfile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_Coch.hdf5\"\n",
    "        train_averagefile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_average.npy\"\n",
    "   \n",
    "    testfile =  \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_Coch.hdf5\"\n",
    "    test_averagefile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_average.npy\"\n",
    "    trainset = h5py.File(trainfile,\"r\")\n",
    "    testset = h5py.File(testfile,\"r\")\n",
    "    \n",
    "    train_coch = trainset[\"/coch\"] #The cochleagrams are in here\n",
    "    test_coch = testset[\"/coch\"]\n",
    "    \n",
    "    #    unbalanced_label_counts = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_label_counts.npy\")\n",
    "    \n",
    "\n",
    "    \n",
    "    if (TASK == \"PREDICT_LABELS\"):\n",
    "        multiple_labels = True\n",
    "        if(music_only): #If we want to use a label set without music\n",
    "            if(unbalanced):\n",
    "                trainlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_train_segments_nomusic_labels_only.hdf5\"           \n",
    "            else:\n",
    "                trainlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/balanced_train_segments_nomusic_labels_only.hdf5\"\n",
    "                \n",
    "            testlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/eval_segments_nomusic_labels_only.hdf5\"\n",
    "\n",
    "            trainlabelsSet = h5py.File(trainlabels_file,\"r\")\n",
    "            testlabelsSet = h5py.File(testlabels_file,\"r\")\n",
    "\n",
    "            trainlabels = trainlabelsSet[\"/labels\"]\n",
    "            testlabels = testlabelsSet[\"/labels\"]  \n",
    "\n",
    "            numlabels = 526\n",
    "        else:\n",
    "            trainlabels = trainset[\"/labels\"]\n",
    "            testlabels = testset[\"/labels\"]\n",
    "\n",
    "            numlabels = 527\n",
    "    elif(TASK == \"PREDICT_COUNTS\"):\n",
    "        multiple_labels = False\n",
    "        if(unbalanced):\n",
    "            trainlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_leaf_label_counts.npy\")\n",
    "        else:\n",
    "            trainlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/balanced_leaf_label_counts.npy\")\n",
    "\n",
    "        testlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/eval_leaf_label_counts.npy\")\n",
    "        numlabels = 11\n",
    "    \n",
    "        \n",
    "    \n",
    "    COCHLEAGRAM_LENGTH = 342000#171 * 880#int(342000) # full is 342000  \n",
    "    train_mean_coch = np.load(train_averagefile)[0:COCHLEAGRAM_LENGTH] #mean cochleagram\n",
    "    #test_mean_coch = np.load(test_averagefile)\n",
    "    \n",
    "    Num_Epochs = 2000\n",
    "    \n",
    "    if(not limit == None):\n",
    "        trainsize = limit\n",
    "        testsize = limit\n",
    "    else:\n",
    "        trainsize = trainset.attrs.get(\"size\")\n",
    "        testsize = testset.attrs.get(\"size\")\n",
    "        limit = \"full\"\n",
    "        \n",
    "    #for unbalanced set    \n",
    "    cutoff = int(trainsize*.90)\n",
    "        \n",
    "    b = 0\n",
    "    batchsize = 100\n",
    "    \n",
    "    \n",
    "    if Freeze_Model_File == None:\n",
    "        Saved_Weights = None\n",
    "        sw = \"F\"\n",
    "    else:\n",
    "        sw = \"T\"\n",
    "        Saved_Weights = get_saved_weights(Freeze_Model_File, COCHLEAGRAM_LENGTH,numlabels,train_mean_coch,Conv1_filtersize,padding, poolmethod,conv1_times_hanning,Learning_Rate,multiple_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"building graph\")\n",
    "    #The training of the graphs are defined here.\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        if OVERIDE_FOLDER == None:\n",
    "        #Get the base graph\n",
    "            if (Net_or_VGG == \"Net\"):\n",
    "                TensorBoard_Folder_train = './{4}/{0}_{6}_{1}_n{2}_lr{3}_conv1FS{5}_m{7}_hcv1{8}_sw{9}_TRAIN'.format(Net_or_VGG,name,limit,Learning_Rate,folder,Conv1_filtersize,TASK,music_only,conv1_times_hanning,sw)\n",
    "                TensorBoard_Folder_test = './{4}/{0}_{6}_{1}_n{2}_lr{3}_conv1FS{5}_m{7}_hcv1{8}_sw{9}_TEST'.format(Net_or_VGG,name,limit,Learning_Rate,folder,Conv1_filtersize,TASK,music_only,conv1_times_hanning,sw)\n",
    "                Saver_Folder = '/{0}_{5}_{1}_n{2}_lr{3}_conv1FS{4}_m{6}_hcv1{7}_sw{8}'.format(Net_or_VGG,name,limit,Learning_Rate,Conv1_filtersize,TASK,music_only,conv1_times_hanning,sw)\n",
    "                nets = Gen_audiosetNet(COCHLEAGRAM_LENGTH,numlabels,train_mean_coch,Conv1_filtersize,padding,poolmethod,conv1_times_hanning,Saved_Weights)\n",
    "            else:\n",
    "                TensorBoard_Folder_train = './{4}/{0}_{4}_{1}_n{2}_lr{3}_TRAIN'.format(Net_or_VGG,name,limit,Learning_Rate,folder,TASK)\n",
    "                TensorBoard_Folder_test = './{4}/{0}_{4}_{1}_n{2}_lr{3}_TEST'.format(Net_or_VGG,name,limit,Learning_Rate,folder,TASK)\n",
    "                Saver_Folder = '/{0}_{4}_{1}_n{2}_lr{3}'.format(Net_or_VGG,name,limit,Learning_Rate,TASK)\n",
    "                nets = Gen_VGG(COCHLEAGRAM_LENGTH,numlabels,train_mean_coch)\n",
    "        else:\n",
    "            TensorBoard_Folder_train = './{0}/{1}_TRAIN'.format(folder,OVERIDE_FOLDER)\n",
    "            TensorBoard_Folder_test = './{0}/{1}_TEST'.format(folder,OVERIDE_FOLDER)\n",
    "            Saver_Folder = '/{0}'.format(OVERIDE_FOLDER)\n",
    "            if (Net_or_VGG == \"Net\"):\n",
    "                nets = Gen_audiosetNet(COCHLEAGRAM_LENGTH,numlabels,train_mean_coch,Conv1_filtersize,padding,poolmethod,conv1_times_hanning,Saved_Weights)\n",
    "            else:\n",
    "                nets = Gen_VGG(COCHLEAGRAM_LENGTH,numlabels,train_mean_coch)\n",
    "        \n",
    "        \n",
    "        #Get the loss functions\n",
    "        Cross_Entropy_Train_on_Labels(nets,numlabels,Learning_Rate,multiple_labels)\n",
    "    \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        #does all the training\n",
    "        if(unbalanced):\n",
    "            train2(test_coch,testlabels,testsize,Saver_Folder,TensorBoard_Folder_train,TensorBoard_Folder_test,batchsize,Num_Epochs,nets,trainsize,train_coch,trainlabels,COCHLEAGRAM_LENGTH,cutoff,TASK=TASK,SAVE = SAVE)      \n",
    "        else:\n",
    "            train1(test_coch,testlabels,testsize,Saver_Folder,TensorBoard_Folder_train,TensorBoard_Folder_test,batchsize,Num_Epochs,nets,trainsize,train_coch,trainlabels,COCHLEAGRAM_LENGTH)\n",
    "        \n",
    "    print(\"done\")\n",
    "            \n",
    "           \n",
    "  \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    debug = True #if true, then use the default csv file to read from. \n",
    "    if debug:\n",
    "        print(\"running\")\n",
    "        learning_rate = 1e-5\n",
    "        \n",
    "        music_only = False \n",
    "        \n",
    "        limit = 2000\n",
    "        Net_or_VGG = \"Net\"\n",
    "        #Net_or_VGG = \"VGG\"\n",
    "        name = \"t3\"\n",
    "        unbalanced = True\n",
    "        folder = \"TB2_1\"\n",
    "        Conv1_filtersize = 9\n",
    "        TASK = \"PREDICT_LABELS\"\n",
    "        #TASK = \"PREDICT_COUNTS\"\n",
    "        OVERIDE_FOLDER = None#\"Net_UNBALANCED91_6_nfull_lr1e-06\"\n",
    "        SAVE = False\n",
    "        padding = \"SAME\"\n",
    "        #padding = \"VALID\"\n",
    "        poolmethod =\"HPOOL\"\n",
    "        #poolmethod =\"MAXPOOL\"\n",
    "        conv1_times_hanning = True\n",
    "        \n",
    "        Freeze_Model_File = \"EPOCH_SAVED/Net_PREDICT_LABELS_A2UN91_5_HP9_nfull_lr1e-05_conv1FS9/model.ckpt-13\"\n",
    "        \n",
    "        main(learning_rate,limit,music_only,Net_or_VGG=Net_or_VGG, name=name,unbalanced=unbalanced,folder=folder,Conv1_filtersize=Conv1_filtersize,TASK=TASK,OVERIDE_FOLDER=OVERIDE_FOLDER,SAVE=SAVE,padding=padding,poolmethod=poolmethod,conv1_times_hanning=conv1_times_hanning,Freeze_Model_File=Freeze_Model_File)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('-l', '--learning_rate' , default = 1e-5)\n",
    "        parser.add_argument('-z', '--limit', default = None)\n",
    "        parser.add_argument('-M', '--Model', default = \"Net\")\n",
    "        parser.add_argument('-n', '--name', default = \"noname\")\n",
    "        parser.add_argument('-f', '--conv1filtersize', default = 9)\n",
    "        parser.add_argument('-t', '--TASK',default = \"PREDICT_LABELS\")\n",
    "        parser.add_argument('-o', '--OVERIDE_FOLDER', default = None)\n",
    "        parser.add_argument('-p', '--poolmethod', default = \"MAXPOOL\")\n",
    "        parser.add_argument('-b', '--tbfolder', default = \"TB2\")\n",
    "        parser.add_argument('-m', '--musiconly', default = None)\n",
    "        parser.add_argument('-x', '--conv1_times_hanning', default = None)\n",
    "        parser.add_argument('-e', '--Freeze_Model_File', default = None)\n",
    "        parser.add_argument('-s', '--save', default = None)\n",
    "        \n",
    "        args = vars(parser.parse_args())\n",
    "\n",
    "        \n",
    "        learning_rate = float(args[\"learning_rate\"])\n",
    "        \n",
    "        if (not args[\"limit\"] == None):\n",
    "            limit = int(args[\"limit\"])\n",
    "        else:\n",
    "            limit = None\n",
    "        \n",
    "        Net_or_VGG = args[\"Model\"]\n",
    "        name = args['name']\n",
    "        Conv1_filtersize = int(args['conv1filtersize'])\n",
    "        TASK = args['TASK']\n",
    "        \n",
    "        if (not args['OVERIDE_FOLDER'] == None):\n",
    "            OVERIDE_FOLDER = args['OVERIDE_FOLDER']\n",
    "        else:\n",
    "            OVERIDE_FOLDER = None\n",
    "        \n",
    "        if (not args['musiconly'] == None):\n",
    "            if (args['musiconly'] == \"True\"):\n",
    "                music_only = True\n",
    "            else:\n",
    "                music_only = False\n",
    "        else:\n",
    "            music_only = False\n",
    "        \n",
    "        \n",
    "        if (not args['conv1_times_hanning'] == None):\n",
    "            if (args['conv1_times_hanning'] == \"True\"):\n",
    "                conv1_times_hanning = True\n",
    "            else:\n",
    "                conv1_times_hanning = False\n",
    "        else:\n",
    "            conv1_times_hanning = False\n",
    "            \n",
    "            \n",
    "        if (not args['save'] == None):\n",
    "            if (args['save'] == \"True\"):\n",
    "                SAVE = True\n",
    "            else:\n",
    "                SAVE = False\n",
    "        else:\n",
    "            SAVE = True\n",
    "        \n",
    "        unbalanced = True\n",
    "        \n",
    "        folder =  args['tbfolder']\n",
    "        poolmethod =args['poolmethod']  \n",
    "        \n",
    "        padding = \"SAME\"\n",
    "        \n",
    "        main(learning_rate,limit,music_only,Net_or_VGG=Net_or_VGG, name=name,unbalanced=unbalanced,folder=folder,Conv1_filtersize=Conv1_filtersize,TASK=TASK,OVERIDE_FOLDER=OVERIDE_FOLDER,SAVE=SAVE,padding=padding,poolmethod=poolmethod,conv1_times_hanning=conv1_times_hanning,Freeze_Model_File)\n",
    "\n",
    "#TODO FIX COCH SIZE EFORE RUNNING ANYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
