{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "beginnning session\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "from math import sqrt\n",
    "import argparse\n",
    "\n",
    "#thanks to https://gist.githubusercontent.com/kukuruza/03731dc494603ceab0c5/raw/3d708320145df0a962cfadb95b3f716b623994e0/gist_cifar10_train.py\n",
    "def put_kernels_on_grid (kernel, grid_Y, grid_X, pad = 1):\n",
    "\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x3 = tf.reshape(x2, tf.stack([grid_X, Y * grid_Y, X, channels])) #3\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x5 = tf.reshape(x4, tf.stack([1, X * grid_X, Y * grid_Y, channels])) #3\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scale to [0, 255] and convert to uint8\n",
    "    return tf.image.convert_image_dtype(x7, dtype = tf.uint8)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        meansum = tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "              stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        stdv = tf.summary.scalar('stddev', stddev)\n",
    "        maxsum = tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        minsum = tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        hist = tf.summary.histogram('histogram', var)\n",
    "        \n",
    "        return tf.summary.merge([meansum,stdv,maxsum,minsum,hist])\n",
    "\n",
    "\n",
    "def main(Learning_Rate,multiplier):\n",
    "\n",
    "    trainfile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_Coch.hdf5\"\n",
    "    train_averagefile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_average.npy\"\n",
    "    testfile = trainfile# \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_Coch.hdf5\"\n",
    "    test_averagefile = train_averagefile# \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_average.npy\"\n",
    "    trainset = h5py.File(trainfile)\n",
    "    testset = h5py.File(testfile)\n",
    "    \n",
    "    train_ave_coch = np.load(train_averagefile)\n",
    "    test_ave_coch = np.load(test_averagefile)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Num_Epochs = 100\n",
    "    \n",
    "    COCHLEAGRAM_LENGTH = int(342000/1) # full is 342000\n",
    "    \n",
    "    limit = 128*multiplier\n",
    "    \n",
    "    trainsize = limit#trainset.attrs.get(\"size\")\n",
    "    testsize = limit#testset.attrs.get(\"size\")\n",
    "    \n",
    "    b = 0\n",
    "    batchsize = 1\n",
    "    \n",
    "    final_filter = 512\n",
    "    \n",
    "    TensorBoard_Folder = './TB/VGGtrain_n{0}_lr{1}'.format(limit,Learning_Rate)\n",
    "    \n",
    "    full_length = final_filter * math.ceil(COCHLEAGRAM_LENGTH/171/2/2/2/2/2)* math.ceil(171/2/2/2/2/2)\n",
    "    \n",
    "    nets = {}\n",
    "    \n",
    "    print(\"starting\")\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            nets['input_to_net'] = tf.placeholder(tf.float32, [None,COCHLEAGRAM_LENGTH], name='input_to_net')\n",
    "            nets['actual_labels'] = tf.placeholder(tf.float32, [None, 527], name='actual_labels')\n",
    "            nets['label_indeces'] = tf.placeholder(tf.int32, [None, 15], name='label_indeces')\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            \n",
    "            nets['accuracy'] = tf.placeholder(tf.float32, (), name='acc')\n",
    "            nets['accsum'] = tf.summary.scalar(\"accuracy\",nets['accuracy'])\n",
    "            \n",
    "            \n",
    "            nets['reshapedCoch'] = tf.reshape(nets['input_to_net'],[-1,171,int(COCHLEAGRAM_LENGTH/171),1], name='reshape_input')\n",
    "\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        #thing = sess.run(reshapedCoch,feed_dict={input_to_net:trainset[\"/coch\"][0:2],actual_labels:trainset[\"/labels\"][0:2]})\n",
    "\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            #Conv_1_1 and save the graphs\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            conv1_1_Weights = weight_variable([3,3,1,64])\n",
    "            \n",
    "            layer1 = variable_summaries(conv1_1_Weights)\n",
    "            \n",
    "            grid = put_kernels_on_grid(conv1_1_Weights,8,8)\n",
    "            nets['conv1_weight_image'] = tf.summary.image('conv1/kernels', grid, max_outputs=3)       \n",
    "            nets['conv1_1'] = conv2d(nets['reshapedCoch'], Weights = conv1_1_Weights, bias = bias_variable([64]), strides=1, name='conv1_1')\n",
    "                \n",
    "            #conv_1_2         \n",
    "            nets['conv1_2'] = conv2d(nets['conv1_1'], Weights = weight_variable([3,3,64,64]), bias = bias_variable([64]), strides=1, name='conv1_2')       \n",
    "            nets['maxpool1'] = maxpool2x2(nets['conv1_2'],k=2 , name='maxpool1')\n",
    "        \n",
    "        with tf.name_scope('conv2'):\n",
    "            nets['conv2_1'] = conv2d(nets['maxpool1'], Weights = weight_variable([3,3,64,128]), bias = bias_variable([128]), strides=1, name='conv2_1')        \n",
    "            nets['conv2_2'] = conv2d(nets['conv2_1'], Weights = weight_variable([3,3,128,128]), bias = bias_variable([128]), strides=1, name='conv2_2')                  \n",
    "            nets['maxpool2'] = maxpool2x2(nets['conv2_2'],k=2 , name='maxpool2')\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('conv3'):\n",
    "            nets['conv3_1'] = conv2d(nets['maxpool2'], Weights = weight_variable([3,3,128,256]), bias = bias_variable([256]), strides=1, name='conv3_1')            \n",
    "            nets['conv3_2'] = conv2d(nets['conv3_1'], Weights = weight_variable([3,3,256,256]), bias = bias_variable([256]), strides=1, name='conv3_2')       \n",
    "            nets['conv3_3'] = conv2d(nets['conv3_2'], Weights = weight_variable([3,3,256,256]), bias = bias_variable([256]), strides=1, name='conv3_3')           \n",
    "            nets['conv3_4'] = conv2d(nets['conv3_3'], Weights = weight_variable([3,3,256,256]), bias = bias_variable([256]), strides=1, name='conv3_4')       \n",
    "        \n",
    "            nets['maxpool3'] = maxpool2x2(nets['conv3_4'],k=2 , name='maxpool3')\n",
    "            \n",
    "        with tf.name_scope('conv4'):\n",
    "            nets['conv4_1'] = conv2d(nets['maxpool3'], Weights = weight_variable([3,3,256,512]), bias = bias_variable([512]), strides=1, name='conv4_1')            \n",
    "            nets['conv4_2'] = conv2d(nets['conv4_1'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv4_2')       \n",
    "            nets['conv4_3'] = conv2d(nets['conv4_2'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv4_3')           \n",
    "            nets['conv4_4'] = conv2d(nets['conv4_3'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv4_4')       \n",
    "        \n",
    "            nets['maxpool4'] = maxpool2x2(nets['conv4_4'],k=2 , name='maxpool4')\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('conv5'):\n",
    "            nets['conv5_1'] = conv2d(nets['maxpool4'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv5_1')            \n",
    "            nets['conv5_2'] = conv2d(nets['conv5_1'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv5_2')       \n",
    "            nets['conv5_3'] = conv2d(nets['conv5_2'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv5_3')           \n",
    "            nets['conv5_4'] = conv2d(nets['conv5_3'], Weights = weight_variable([3,3,512,512]), bias = bias_variable([512]), strides=1, name='conv5_4')       \n",
    "        \n",
    "            nets['maxpool5'] = maxpool2x2(nets['conv5_4'],k=2 , name='maxpool5')\n",
    "            \n",
    "            nets['flattened'] = tf.reshape(nets['maxpool5'], [-1, full_length])\n",
    "        \n",
    "        with tf.name_scope('fc_1'):\n",
    "            W_fc1 = weight_variable([full_length, 4096]) # 4,959,232\n",
    "            b_fc1 = bias_variable([4096])\n",
    "            nets['fc_1'] = tf.nn.relu(tf.matmul(nets['flattened'], W_fc1) + b_fc1)\n",
    "            nets['h_fc1_drop'] = tf.nn.dropout(nets['fc_1'], keep_prob)\n",
    "        \n",
    "        with tf.name_scope('fc_2'):\n",
    "            W_fc2 = weight_variable([4096, 4096]) # 4,959,232\n",
    "            b_fc2 = bias_variable([4096])\n",
    "            nets['fc_2'] = tf.nn.relu(tf.matmul(nets['h_fc1_drop'], W_fc2) + b_fc2)\n",
    "            nets['h_fc2_drop'] = tf.nn.dropout(nets['fc_2'], keep_prob)\n",
    "            \n",
    "        with tf.name_scope('fc_3'):\n",
    "            W_fc3 = weight_variable([4096, 527],name='W_fc3') # 4,959,232\n",
    "            b_fc3 = bias_variable([527],name = 'b_fc3')\n",
    "            nets['predicted_labels'] = tf.nn.relu(tf.matmul(nets['h_fc2_drop'], W_fc3) + b_fc3)\n",
    "\n",
    "            \n",
    "        with tf.variable_scope(\"eval\"):\n",
    "            nets['cross_entroy'] = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels =nets['actual_labels'],logits = nets['predicted_labels'] ))\n",
    "            nets['train_step'] = tf.train.AdamOptimizer(Learning_Rate).minimize(nets['cross_entroy'])\n",
    "\n",
    "            nets['cross_entropy_summary'] = tf.summary.scalar(\"cross_entropy_summary\",nets['cross_entroy'])\n",
    "            \n",
    "            #num = np.sum(nets['actual_labelsi'])\n",
    "            #nets['accuracy'] = tf.metrics.mean(tf.nn.in_top_k(nets['predicted_labels'],nets['actual_labelsi'],2))\n",
    "\n",
    "            #calculating accuracys\n",
    "\n",
    "            _,nets['indeces'] = tf.nn.top_k(nets['predicted_labels'],15)\n",
    "            prediction_sums = tf.summary.histogram('predicted_labels_histogram',nets['indeces'])\n",
    "            nets['numCorrect'] = tf.shape(tf.sets.set_intersection(nets['indeces'],nets['label_indeces'],False).values)[0]\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        print(\"beginnning session\")\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer = tf.summary.FileWriter(TensorBoard_Folder, sess.graph)\n",
    "            \n",
    "            for epoch in range(Num_Epochs):\n",
    "                print(\"epoch: \",epoch)\n",
    "                print(\"training\")\n",
    "                \n",
    "                #Train a full epoch\n",
    "                startindex = 0\n",
    "                endindex = startindex+batchsize\n",
    "                while(endindex <= trainsize):\n",
    "                    print(startindex)\n",
    "                    _,cross_entropySummary,l1Sums = sess.run([nets['train_step'],nets['cross_entropy_summary'],layer1],feed_dict={nets['input_to_net']:trainset[\"/coch\"][startindex:endindex][:,0:COCHLEAGRAM_LENGTH]-train_ave_coch[0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainset[\"/labels\"][startindex:endindex],keep_prob:.5})\n",
    "                    train_writer.add_summary(cross_entropySummary, epoch)\n",
    "                    train_writer.add_summary(l1Sums, epoch)\n",
    "                    \n",
    "                    startindex = endindex\n",
    "                    endindex = startindex+batchsize\n",
    "                    \n",
    "                \n",
    "                print(\"eval\")\n",
    "                #Evaluate the full epoch\n",
    "                startindex = 0\n",
    "                endindex = startindex+batchsize\n",
    "                total_count = 0\n",
    "                correct = 0\n",
    "                while(endindex <= testsize):\n",
    "                    print(startindex)\n",
    "                    trainBatchLabels = testset[\"/labels\"][startindex:endindex]\n",
    "                    indeces_list = []\n",
    "                    for row in trainBatchLabels:\n",
    "                        indeces= np.where(row==1)[0]\n",
    "                        total_count = total_count + len(indeces)\n",
    "                        indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "                        \n",
    "                    \n",
    "                    addme,pred_sum = sess.run([nets['numCorrect'],prediction_sums],feed_dict={nets['input_to_net']:testset[\"/coch\"][startindex:endindex][:,0:COCHLEAGRAM_LENGTH]-test_ave_coch[0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testset[\"/labels\"][startindex:endindex],nets['label_indeces']:indeces_list,keep_prob:1})\n",
    "                    train_writer.add_summary(pred_sum, epoch + startindex/testsize)\n",
    "                    correct = correct + addme\n",
    "                    \n",
    "                    startindex = endindex\n",
    "                    endindex = startindex+batchsize\n",
    "                \n",
    "                \n",
    "                summary = sess.run(nets['accsum'],feed_dict={nets['accuracy']:correct/total_count})\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "                \n",
    "                image_conv1 = sess.run(nets['conv1_weight_image'])\n",
    "                train_writer.add_summary(image_conv1, epoch)\n",
    "    \n",
    "\n",
    "                \n",
    "                startindex = random.randint(0,testsize-batchsize)\n",
    "                endindex = startindex+batchsize\n",
    "                potato = sess.run(nets['indeces'],feed_dict={nets['input_to_net']:testset[\"/coch\"][startindex:endindex][:,0:COCHLEAGRAM_LENGTH]-test_ave_coch[0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testset[\"/labels\"][startindex:endindex],nets['label_indeces']:indeces_list,keep_prob:1})\n",
    "                #print()\n",
    "                #print(\"Epoch {0}, accuracy {1}\".format(epoch,correct/total_count))\n",
    "                #print(start)\n",
    "                #print(end)\n",
    "                #print(total_count)\n",
    "                #print(correct)\n",
    "                #print(\"actual: \", indeces_list)\n",
    "                #print(\"predicted :\", potato)\n",
    "                #print()\n",
    "                #f= open(\"output3.csv\",\"a\")\n",
    "                #f.write(\"loop {0}, accuracy {1}\\n\".format(i,correct/total_count))\n",
    "                #f.close()''\n",
    "                \n",
    "    print(\"done\")\n",
    "            \n",
    "           \n",
    "    \n",
    "def weight_variable(shape, name = None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape,name = None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "#builds the components\n",
    "def conv2d(inputtensor, Weights, bias, strides=1,name = None):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(inputtensor, Weights, strides=[1, strides, strides, 1], padding='SAME',name=name)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2x2(x, k=2, name = None):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME',name=name)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    debug = True #if true, then use the default csv file to read from. \n",
    "    if debug:\n",
    "        learning_rate = 1e-4\n",
    "        multiplier = 1\n",
    "        main(learning_rate,multiplier)\n",
    "    else:\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('-a', '--learning_rate')\n",
    "        parser.add_argument('-b', '--multiplier')\n",
    "\n",
    "        args = vars(parser.parse_args())\n",
    "\n",
    "        learning_rate = float(args[\"learning_rate\"])\n",
    "        multiplier = int(args[\"multiplier\"])\n",
    "        \n",
    "        print(learning_rate)\n",
    "        print(multiplier)\n",
    "\n",
    "        main(learning_rate,multiplier)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
