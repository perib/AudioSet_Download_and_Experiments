{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO SAVE PER EPOCH IN NON UNBALANCED SET\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "from math import sqrt\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Generates and returns the VGG graph (up to the predictions node)\n",
    "def Gen_VGG(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch, padding=\"SAME\"):\n",
    "    final_filter = 512\n",
    "    full_length = final_filter * math.ceil(COCHLEAGRAM_LENGTH / 171 / 2 / 2 / 2 / 2 / 2) * math.ceil(\n",
    "        171 / 2 / 2 / 2 / 2 / 2)\n",
    "    print(full_length)\n",
    "    nets = {}\n",
    "    with tf.name_scope('input'):\n",
    "        nets['input_to_net'] = tf.placeholder(tf.float32, [None, COCHLEAGRAM_LENGTH], name='input_to_net')\n",
    "        mean_tensor = tf.constant(train_mean_coch, dtype=tf.float32)\n",
    "        nets['subtract_mean'] = tf.subtract(nets['input_to_net'], mean_tensor, name='Subtract_Mean')\n",
    "        nets['reshapedCoch'] = tf.reshape(nets['subtract_mean'], [-1, 171, int(COCHLEAGRAM_LENGTH / 171), 1],\n",
    "                                          name='reshape_input')\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        nets['accuracy'] = tf.placeholder(tf.float32, (), name='acc')\n",
    "        nets['accsum'] = tf.summary.scalar(\"accuracy\", nets['accuracy'])\n",
    "\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "    # thing = sess.run(reshapedCoch,feed_dict={input_to_net:train_coch[0:2],actual_labels:trainlabels[0:2]})\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        # Conv_1_1 and save the graphs\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        nets['conv1_1_Weights'] = weight_variable([3, 3, 1, 64])\n",
    "\n",
    "        nets['layer1'] = variable_summaries(nets['conv1_1_Weights'])\n",
    "\n",
    "        nets['grid'] = put_kernels_on_grid(nets['conv1_1_Weights'], 8, 8)\n",
    "        nets['conv1_weight_image'] = tf.summary.image('conv1/kernels', nets['grid'], max_outputs=3)\n",
    "        nets['conv1_1'] = conv2d(nets['reshapedCoch'], Weights=nets['conv1_1_Weights'], bias=bias_variable([64]),\n",
    "                                 strides=1, name='conv1_1')\n",
    "\n",
    "        # conv_1_2\n",
    "        nets['conv1_2'] = conv2d(nets['conv1_1'], Weights=weight_variable([3, 3, 64, 64]), bias=bias_variable([64]),\n",
    "                                 strides=1, name='conv1_2')\n",
    "\n",
    "    with tf.name_scope(\"maxpool1\"):\n",
    "        nets['maxpool1'] = maxpool2x2(nets['conv1_2'], k=2, name='maxpool1', padding=padding)\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        nets['conv2_1'] = conv2d(nets['maxpool1'], Weights=weight_variable([3, 3, 64, 128]), bias=bias_variable([128]),\n",
    "                                 strides=1, name='conv2_1')\n",
    "        nets['conv2_2'] = conv2d(nets['conv2_1'], Weights=weight_variable([3, 3, 128, 128]), bias=bias_variable([128]),\n",
    "                                 strides=1, name='conv2_2')\n",
    "    with tf.name_scope(\"maxpool2\"):\n",
    "        nets['maxpool2'] = maxpool2x2(nets['conv2_2'], k=2, name='maxpool2', padding=padding)\n",
    "\n",
    "    with tf.name_scope('conv3'):\n",
    "        nets['conv3_1'] = conv2d(nets['maxpool2'], Weights=weight_variable([3, 3, 128, 256]), bias=bias_variable([256]),\n",
    "                                 strides=1, name='conv3_1')\n",
    "        nets['conv3_2'] = conv2d(nets['conv3_1'], Weights=weight_variable([3, 3, 256, 256]), bias=bias_variable([256]),\n",
    "                                 strides=1, name='conv3_2')\n",
    "        nets['conv3_3'] = conv2d(nets['conv3_2'], Weights=weight_variable([3, 3, 256, 256]), bias=bias_variable([256]),\n",
    "                                 strides=1, name='conv3_3')\n",
    "        nets['conv3_4'] = conv2d(nets['conv3_3'], Weights=weight_variable([3, 3, 256, 256]), bias=bias_variable([256]),\n",
    "                                 strides=1, name='conv3_4')\n",
    "    with tf.name_scope(\"maxpool3\"):\n",
    "        nets['maxpool3'] = maxpool2x2(nets['conv3_4'], k=2, name='maxpool3', padding=padding)\n",
    "\n",
    "    with tf.name_scope('conv4'):\n",
    "        nets['conv4_1'] = conv2d(nets['maxpool3'], Weights=weight_variable([3, 3, 256, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv4_1')\n",
    "        nets['conv4_2'] = conv2d(nets['conv4_1'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv4_2')\n",
    "        nets['conv4_3'] = conv2d(nets['conv4_2'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv4_3')\n",
    "        nets['conv4_4'] = conv2d(nets['conv4_3'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv4_4')\n",
    "\n",
    "    with tf.name_scope(\"maxpool4\"):\n",
    "        nets['maxpool4'] = maxpool2x2(nets['conv4_4'], k=2, name='maxpool4', padding=padding)\n",
    "\n",
    "    with tf.name_scope('conv5'):\n",
    "        nets['conv5_1'] = conv2d(nets['maxpool4'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv5_1')\n",
    "        nets['conv5_2'] = conv2d(nets['conv5_1'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv5_2')\n",
    "        nets['conv5_3'] = conv2d(nets['conv5_2'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv5_3')\n",
    "        nets['conv5_4'] = conv2d(nets['conv5_3'], Weights=weight_variable([3, 3, 512, 512]), bias=bias_variable([512]),\n",
    "                                 strides=1, name='conv5_4')\n",
    "\n",
    "    with tf.name_scope(\"maxpool5\"):\n",
    "        nets['maxpool5'] = maxpool2x2(nets['conv5_4'], k=2, name='maxpool5', padding=padding)\n",
    "\n",
    "    with tf.name_scope(\"flatten\"):\n",
    "        nets['flattened'] = tf.reshape(nets['maxpool5'], [-1, full_length])\n",
    "\n",
    "    with tf.name_scope(\"keep_prob\"):\n",
    "        nets['keep_prob'] = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope('fc_1'):\n",
    "        W_fc1 = weight_variable([full_length, 4096])  # 4,959,232\n",
    "        b_fc1 = bias_variable([4096])\n",
    "        nets['fc_1'] = tf.nn.relu(tf.matmul(nets['flattened'], W_fc1) + b_fc1)\n",
    "        nets['h_fc1_drop'] = tf.nn.dropout(nets['fc_1'], nets['keep_prob'])\n",
    "\n",
    "    with tf.name_scope('fc_2'):\n",
    "        W_fc2 = weight_variable([4096, 4096])  # 4,959,232\n",
    "        b_fc2 = bias_variable([4096])\n",
    "        nets['fc_2'] = tf.nn.relu(tf.matmul(nets['h_fc1_drop'], W_fc2) + b_fc2)\n",
    "        nets['h_fc2_drop'] = tf.nn.dropout(nets['fc_2'], nets['keep_prob'])\n",
    "\n",
    "    with tf.name_scope('fc_3'):\n",
    "        W_fc3 = weight_variable([4096, numlabels], name='W_fc3')  # 4,959,232\n",
    "        b_fc3 = bias_variable([numlabels], name='b_fc3')\n",
    "        nets['predicted_labels'] = tf.nn.relu(tf.matmul(nets['h_fc2_drop'], W_fc3) + b_fc3)\n",
    "\n",
    "    return nets\n",
    "\n",
    "\n",
    "# Generates and returns our own graph (up to the predictions node)\n",
    "def Gen_audiosetNet(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch, Conv1_filtersize=9, padding=\"SAME\",\n",
    "                    poolmethod=\"MAXPOOL\"):\n",
    "    variable_list = []\n",
    "\n",
    "    conv1_strides = 3\n",
    "    conv2_strides = 2\n",
    "    conv3_strides = 1\n",
    "    conv4_strides = 1\n",
    "    conv5_strides = 1\n",
    "    final_filter = 512\n",
    "    full_length = final_filter * math.ceil(\n",
    "        COCHLEAGRAM_LENGTH / 171 / conv1_strides / conv2_strides / conv3_strides / conv4_strides / conv5_strides / 4 / 2) * math.ceil(\n",
    "        171 / conv1_strides / conv2_strides / conv3_strides / conv4_strides / conv5_strides / 4 / 2)\n",
    "\n",
    "    nets = {}\n",
    "    with tf.name_scope('input'):\n",
    "        nets['input_to_net'] = tf.placeholder(tf.float32, [None, COCHLEAGRAM_LENGTH], name='input_to_net')\n",
    "        mean_tensor = tf.constant(train_mean_coch, dtype=tf.float32)\n",
    "        nets['subtract_mean'] = tf.subtract(nets['input_to_net'], mean_tensor, name='Subtract_Mean')\n",
    "        nets['reshapedCoch'] = tf.reshape(nets['subtract_mean'], [-1, 171, int(COCHLEAGRAM_LENGTH / 171), 1],\n",
    "                                          name='reshape_input')\n",
    "\n",
    "        # print(\"input \", nets['reshapedCoch'].shape)\n",
    "\n",
    "    with tf.name_scope(\"images\"):\n",
    "        nets['coch_images'] = tf.summary.image('image', nets['reshapedCoch'][0:5, :, :, :])\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        nets['accuracy'] = tf.placeholder(tf.float32, (), name='acc')\n",
    "        nets['accsum'] = tf.summary.scalar(\"accuracy\", nets['accuracy'])\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        nets['conv1_Weights'] = weight_variable([Conv1_filtersize, Conv1_filtersize, 1, 96], name='conv1_Weights')\n",
    "        nets['conv1_bias'] = bias_variable([96], name='bias1_Weights')\n",
    "        nets['layer1'] = variable_summaries(nets['conv1_Weights'])\n",
    "        nets['grid'] = put_kernels_on_grid(nets['conv1_Weights'], 16, 6)\n",
    "        nets['conv1_weight_image'] = tf.summary.image('conv1/kernels', nets['grid'], max_outputs=3)\n",
    "        nets['conv1'] = tf.nn.local_response_normalization(\n",
    "            conv2d(nets['reshapedCoch'], Weights=nets['conv1_Weights'], bias=nets['conv1_bias'], strides=conv1_strides,\n",
    "                   name='conv1', padding=padding), depth_radius=2)\n",
    "\n",
    "    with tf.name_scope(\"conv1_summaries\"):\n",
    "        sum_list = []\n",
    "        for im in range(2):\n",
    "            for channel in range(2):\n",
    "                sum_list.append(tf.summary.image('conv1/featuremaps_im_{0}_ch{0}'.format(im, channel),\n",
    "                                                 nets['conv1'][im:im + 1, :, :, channel:channel + 1], max_outputs=3))\n",
    "                sum_list.append(tf.summary.image('conv1/weight_im{0}_ch{0}'.format(im, channel),\n",
    "                                                 tf.transpose(nets['conv1_Weights'], (2, 0, 1, 3))[0:1, :, :,\n",
    "                                                 channel:channel + 1], max_outputs=3))\n",
    "                sum_list.append(tf.summary.histogram('conv1/featuremaps_hist_im_{0}_ch{0}'.format(im, channel),\n",
    "                                                     nets['conv1'][im:im + 1, :, :, channel:channel + 1]))\n",
    "        nets['conv1_sums'] = tf.summary.merge(sum_list)\n",
    "\n",
    "        # print(\"conv1\", nets['conv1'].shape)\n",
    "\n",
    "    if poolmethod == \"MAXPOOL\":\n",
    "        with tf.name_scope(\"maxpool1\"):\n",
    "            nets['pool1'] = maxpool2x2(nets['conv1'], k=2, name='maxpool1', padding=\"SAME\")\n",
    "    elif poolmethod == \"HPOOL\":\n",
    "        with tf.name_scope(\"h_pool1\"):\n",
    "            nets['pool1'] = add_hanning_pooling(nets, 'conv1', downsample=2, length_of_window=8, layer_name='pool1',\n",
    "                                                make_plots=False)\n",
    "\n",
    "            # print(\"pool1, \", nets['pool1'].shape)\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        nets['conv2_Weights'] = weight_variable([5, 5, 96, 256], name='conv2_Weights')\n",
    "        nets['conv2'] = tf.nn.local_response_normalization(\n",
    "            conv2d(nets['pool1'], Weights=nets['conv2_Weights'], bias=bias_variable([256], name='conv2_bias'),\n",
    "                   strides=conv2_strides, name='conv2', padding=padding), depth_radius=2)\n",
    "\n",
    "        # print(\"conv2 \", nets['conv2'].shape)\n",
    "\n",
    "    if poolmethod == \"MAXPOOL\":\n",
    "        with tf.name_scope(\"maxpool2\"):\n",
    "            nets['pool2'] = maxpool2x2(nets['conv2'], k=2, name='maxpool2', padding=\"SAME\")\n",
    "    elif poolmethod == \"HPOOL\":\n",
    "        with tf.name_scope(\"h_pool2\"):\n",
    "            nets['pool2'] = add_hanning_pooling(nets, 'conv2', downsample=2, length_of_window=8, layer_name='pool2',\n",
    "                                                make_plots=False)\n",
    "\n",
    "    # print(\"pool2, \", nets['pool2'].shape)\n",
    "\n",
    "    with tf.name_scope('conv3'):\n",
    "        nets['conv3'] = conv2d(nets['pool2'], Weights=weight_variable([3, 3, 256, 512], name='conv3_Weights'),\n",
    "                               bias=bias_variable([512], name='conv3_bias'), strides=conv3_strides, name='conv3',\n",
    "                               padding=padding)\n",
    "        # print(\"conv3 \", nets['conv3'].shape)\n",
    "\n",
    "    with tf.name_scope('conv4'):\n",
    "        nets['conv4'] = conv2d(nets['conv3'], Weights=weight_variable([3, 3, 512, 1024], name='conv4_Weights'),\n",
    "                               bias=bias_variable([1024], name='conv4_bias'), strides=conv4_strides, name='conv4',\n",
    "                               padding=padding)\n",
    "        # print(\"conv4\", nets['conv4'].shape)\n",
    "\n",
    "    with tf.name_scope('conv5'):\n",
    "        nets['conv5'] = conv2d(nets['conv4'], Weights=weight_variable([3, 3, 1024, final_filter], name='conv5_Weights'),\n",
    "                               bias=bias_variable([final_filter], name='conv5_bias'), strides=conv5_strides,\n",
    "                               name='conv5', padding=padding)\n",
    "\n",
    "        # print(\"conv5\",nets['conv5'].shape )\n",
    "\n",
    "    if poolmethod == \"MAXPOOL\":\n",
    "        with tf.name_scope(\"maxpool3\"):\n",
    "            nets['pool3'] = maxpool2x2(nets['conv5'], k=2, name='maxpool3', padding=\"SAME\")\n",
    "    elif poolmethod == \"HPOOL\":\n",
    "        with tf.name_scope(\"h_pool3\"):\n",
    "            nets['pool3'] = add_hanning_pooling(nets, 'conv5', downsample=2, length_of_window=8, layer_name='pool3',\n",
    "                                                make_plots=False)\n",
    "\n",
    "    with tf.name_scope(\"flatten\"):\n",
    "        nets['flattened'] = tf.reshape(nets['pool3'], [-1, full_length])\n",
    "\n",
    "    with tf.name_scope('fc_1'):\n",
    "        W_fc1 = weight_variable([full_length, 1024])  # 4,959,232\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        nets['fc_1'] = tf.nn.relu(tf.matmul(nets['flattened'], W_fc1) + b_fc1)\n",
    "\n",
    "        nets['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        nets['h_fc1_drop'] = tf.nn.dropout(nets['fc_1'], nets['keep_prob'])\n",
    "\n",
    "    with tf.name_scope('fc_2'):\n",
    "        W_fc2 = weight_variable([1024, numlabels])\n",
    "        b_fc2 = bias_variable([numlabels])\n",
    "        nets['predicted_labels'] = tf.matmul(nets['h_fc1_drop'], W_fc2) + b_fc2\n",
    "\n",
    "    return nets\n",
    "\n",
    "\n",
    "def Cross_Entropy_Train_on_Labels(nets, numlabels, Learning_Rate, multiple_labels=True):\n",
    "    # The evaluation/training of the graph\n",
    "    with tf.variable_scope(\"Cross_Entropy\"):\n",
    "        nets['actual_labels'] = tf.placeholder(tf.float32, [None, numlabels], name='actual_labels')\n",
    "\n",
    "        if multiple_labels:\n",
    "            nets['cross_entroy'] = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(labels=nets['actual_labels'], logits=nets['predicted_labels']))\n",
    "        else:\n",
    "            nets['cross_entroy'] = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=nets['actual_labels'], logits=nets['predicted_labels']))\n",
    "            # nets['cross_entroy'] = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels =nets['actual_labels'],logits = nets['predicted_labels'] ))\n",
    "\n",
    "    with tf.variable_scope(\"TrainStep\"):\n",
    "        '''\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        epsilon=1e-08,'''\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-08\n",
    "        nets['global_step'] = tf.Variable(0, name='global_step', trainable=False)\n",
    "        nets['train_step'] = tf.train.AdamOptimizer(Learning_Rate).minimize(nets['cross_entroy'],\n",
    "                                                                            global_step=nets['global_step'])\n",
    "        # nets['train_step'] = tf.train.AdamOptimizer(learning_rate=Learning_Rate,beta1=beta1,beta2=beta2,epsilon=epsilon).minimize(nets['cross_entroy'])\n",
    "\n",
    "        # num = np.sum(nets['actual_labelsi'])\n",
    "        # nets['accuracy'] = tf.metrics.mean(tf.nn.in_top_k(nets['predicted_labels'],nets['actual_labelsi'],2))\n",
    "\n",
    "        # calculating accuracy\n",
    "    with tf.name_scope(\"Summaries\"):\n",
    "        nets['label_indeces'] = tf.placeholder(tf.int32, [None, 15], name='label_indeces')\n",
    "\n",
    "        _, nets['indeces'] = tf.nn.top_k(nets['predicted_labels'], 15)\n",
    "        prediction_sums = tf.summary.histogram('predicted_labels_histogram', nets['indeces'])\n",
    "        nets['numCorrect'] = tf.shape(tf.sets.set_intersection(nets['indeces'], nets['label_indeces'], False).values)[0]\n",
    "\n",
    "        _, nets['Top_1_index'] = tf.nn.top_k(nets['predicted_labels'], 1)\n",
    "        nets['numCorrect_top_pred'] = \\\n",
    "        tf.shape(tf.sets.set_intersection(nets['Top_1_index'], nets['label_indeces'], False).values)[0]\n",
    "\n",
    "    with tf.name_scope(\"Cross_Entropy_Loss\"):\n",
    "        nets['xe_ave'] = tf.placeholder(tf.float32, (), name='xe_ave')\n",
    "        nets['xesum'] = tf.summary.scalar(\"Test_cross_entropy_loss_sample\", nets['xe_ave'])\n",
    "        nets['cross_entropy_summary'] = tf.summary.scalar(\"Train_cross_entropy_loss\", nets['cross_entroy'])\n",
    "\n",
    "        nets['xe_avet'] = tf.placeholder(tf.float32, (), name='xe_avet')\n",
    "        nets['xesumt'] = tf.summary.scalar(\"Train_cross_entropy_loss_sample\", nets['xe_avet'])\n",
    "\n",
    "    with tf.name_scope(\"Count_Tracker\"):\n",
    "        nets['current_epoch'] = tf.Variable(0, name='current_epoch', trainable=False, dtype=tf.int32)\n",
    "        nets['increment_current_epoch'] = tf.assign(nets['current_epoch'], nets['current_epoch'] + 1)\n",
    "\n",
    "        nets['current_step'] = tf.Variable(0, name='current_step', trainable=False, dtype=tf.int32)\n",
    "        nets['increment_current_step'] = tf.assign(nets['current_step'], nets['current_step'] + 1)\n",
    "        nets['reset_current_step'] = tf.assign(nets['current_step'], 0)\n",
    "\n",
    "\n",
    "# thanks to https://gist.githubusercontent.com/kukuruza/03731dc494603ceab0c5/raw/3d708320145df0a962cfadb95b3f716b623994e0/gist_cifar10_train.py\n",
    "def put_kernels_on_grid(kernel, grid_Y, grid_X, pad=1):\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant([[pad, pad], [pad, pad], [0, 0], [0, 0]]), mode='CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x3 = tf.reshape(x2, tf.stack([grid_X, Y * grid_Y, X, channels]))  # 3\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x5 = tf.reshape(x4, tf.stack([1, X * grid_X, Y * grid_Y, channels]))  # 3\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scale to [0, 255] and convert to uint8\n",
    "    return tf.image.convert_image_dtype(x7, dtype=tf.uint8)\n",
    "\n",
    "\n",
    "# a bunch of neat stats we can summarize\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        meansum = tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        stdv = tf.summary.scalar('stddev', stddev)\n",
    "        maxsum = tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        minsum = tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        hist = tf.summary.histogram('histogram', var)\n",
    "\n",
    "        return tf.summary.merge([meansum, stdv, maxsum, minsum, hist])\n",
    "\n",
    "\n",
    "def main(Learning_Rate, limit=None, music_only=False, print_extras=True, Net_or_VGG=\"Net\", name=\"\", unbalanced=False,\n",
    "         folder=\"TB2\", Conv1_filtersize=9, TASK=\"PREDICT_LABELS\", OVERIDE_FOLDER=None, SAVE=True, padding=\"SAME\",\n",
    "         poolmethod=\"MAXPOOL\"):\n",
    "    # if we want to use the unbalanced set for training/eval, read that file in\n",
    "    if (unbalanced):\n",
    "        trainfile = \"/om/user/ribeirop/audiosetDL/unbalanced_stripped/unbalanced_train_segments_Coch.hdf5\"\n",
    "        train_averagefile = \"/om/user/ribeirop/audiosetDL/unbalanced_stripped/unbalanced_train_segments_average.npy\"\n",
    "    else:\n",
    "        trainfile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_Coch.hdf5\"\n",
    "        train_averagefile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_average.npy\"\n",
    "\n",
    "    testfile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_Coch.hdf5\"\n",
    "    test_averagefile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_average.npy\"\n",
    "    trainset = h5py.File(trainfile, \"r\")\n",
    "    testset = h5py.File(testfile, \"r\")\n",
    "\n",
    "    train_coch = trainset[\"/coch\"]  # The cochleagrams are in here\n",
    "    test_coch = testset[\"/coch\"]\n",
    "\n",
    "    #    unbalanced_label_counts = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_label_counts.npy\")\n",
    "\n",
    "\n",
    "    if (TASK == \"PREDICT_LABELS\"):\n",
    "        multiple_labels = True\n",
    "        if (music_only):  # If we want to use a label set without music\n",
    "            if (unbalanced):\n",
    "                trainlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_train_segments_nomusic_labels_only.hdf5\"\n",
    "            else:\n",
    "                trainlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/balanced_train_segments_nomusic_labels_only.hdf5\"\n",
    "\n",
    "            testlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/eval_segments_nomusic_labels_only.hdf5\"\n",
    "\n",
    "            trainlabelsSet = h5py.File(trainlabels_file, \"r\")\n",
    "            testlabelsSet = h5py.File(testlabels_file, \"r\")\n",
    "\n",
    "            trainlabels = trainlabelsSet[\"/labels\"]\n",
    "            testlabels = testlabelsSet[\"/labels\"]\n",
    "\n",
    "            numlabels = 525\n",
    "        else:\n",
    "            trainlabels = trainset[\"/labels\"]\n",
    "            testlabels = testset[\"/labels\"]\n",
    "\n",
    "            numlabels = 527\n",
    "    elif (TASK == \"PREDICT_COUNTS\"):\n",
    "        multiple_labels = False\n",
    "        if (unbalanced):\n",
    "            trainlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_label_counts.npy\")\n",
    "        else:\n",
    "            trainlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/balanced_label_counts.npy\")\n",
    "\n",
    "        testlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/eval_label_counts.npy\")\n",
    "        numlabels = 15\n",
    "\n",
    "    COCHLEAGRAM_LENGTH = int(342000)  # full is 342000\n",
    "    train_mean_coch = np.load(train_averagefile)[0:COCHLEAGRAM_LENGTH]  # mean cochleagram\n",
    "    # test_mean_coch = np.load(test_averagefile)\n",
    "\n",
    "    Num_Epochs = 2000\n",
    "\n",
    "    if (not limit == None):\n",
    "        trainsize = limit\n",
    "        testsize = limit\n",
    "    else:\n",
    "        trainsize = trainset.attrs.get(\"size\")\n",
    "        testsize = testset.attrs.get(\"size\")\n",
    "        limit = \"full\"\n",
    "\n",
    "    # for unbalanced set\n",
    "    cutoff = int(trainsize * .90)\n",
    "\n",
    "    b = 0\n",
    "    batchsize = 100\n",
    "\n",
    "    for epoch_step in range(6, 15):\n",
    "        epoch_step_folder = \"EPOCH_SAVED/{0}/model.ckpt-{1}\".format(OVERIDE_FOLDER, epoch_step)\n",
    "\n",
    "        print(epoch_step_folder)\n",
    "\n",
    "        print(\"building graph\")\n",
    "        # The training of the graphs are defined here.\n",
    "        with tf.Graph().as_default():\n",
    "\n",
    "            if OVERIDE_FOLDER == None:\n",
    "                # Get the base graph\n",
    "                if (Net_or_VGG == \"Net\"):\n",
    "                    TensorBoard_Folder_train = './{4}/{0}_{6}_{1}_n{2}_lr{3}_conv1FS{5}_TRAIN'.format(Net_or_VGG, name,\n",
    "                                                                                                      limit,\n",
    "                                                                                                      Learning_Rate,\n",
    "                                                                                                      folder,\n",
    "                                                                                                      Conv1_filtersize,\n",
    "                                                                                                      TASK)\n",
    "                    TensorBoard_Folder_test = './{4}/{0}_{6}_{1}_n{2}_lr{3}_conv1FS{5}_TEST'.format(Net_or_VGG, name,\n",
    "                                                                                                    limit,\n",
    "                                                                                                    Learning_Rate,\n",
    "                                                                                                    folder,\n",
    "                                                                                                    Conv1_filtersize,\n",
    "                                                                                                    TASK)\n",
    "                    Saver_Folder = '/{0}_{5}_{1}_n{2}_lr{3}_conv1FS{4}'.format(Net_or_VGG, name, limit, Learning_Rate,\n",
    "                                                                               Conv1_filtersize, TASK)\n",
    "                    nets = Gen_audiosetNet(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch, Conv1_filtersize, padding,\n",
    "                                           poolmethod)\n",
    "                else:\n",
    "                    TensorBoard_Folder_train = './{4}/{0}_{4}_{1}_n{2}_lr{3}_TRAIN'.format(Net_or_VGG, name, limit,\n",
    "                                                                                           Learning_Rate, folder, TASK)\n",
    "                    TensorBoard_Folder_test = './{4}/{0}_{4}_{1}_n{2}_lr{3}_TEST'.format(Net_or_VGG, name, limit,\n",
    "                                                                                         Learning_Rate, folder, TASK)\n",
    "                    Saver_Folder = '/{0}_{4}_{1}_n{2}_lr{3}'.format(Net_or_VGG, name, limit, Learning_Rate, TASK)\n",
    "                    nets = Gen_VGG(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch)\n",
    "            else:\n",
    "                TensorBoard_Folder_train = './{0}/{1}_TRAIN'.format(folder, OVERIDE_FOLDER)\n",
    "                TensorBoard_Folder_test = './{0}/{1}_TEST'.format(folder, OVERIDE_FOLDER)\n",
    "                Saver_Folder = '/{0}'.format(OVERIDE_FOLDER)\n",
    "                if (Net_or_VGG == \"Net\"):\n",
    "                    nets = Gen_audiosetNet(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch, Conv1_filtersize, padding,\n",
    "                                           poolmethod)\n",
    "                else:\n",
    "                    nets = Gen_VGG(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch)\n",
    "\n",
    "            print(TensorBoard_Folder_train)\n",
    "            print(TensorBoard_Folder_test)\n",
    "            # Get the loss functions\n",
    "            Cross_Entropy_Train_on_Labels(nets, numlabels, Learning_Rate, multiple_labels)\n",
    "\n",
    "            merged = tf.summary.merge_all()\n",
    "\n",
    "            # CALC MISSING CROSS ENTROPY\n",
    "\n",
    "\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            with tf.Session() as sess:\n",
    "                saver.restore(sess, epoch_step_folder)\n",
    "                offset = 0#random.randint(0,100*2000)\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    TensorBoard_Folder_train)  # tensorboard writters for both train and test accuracy\n",
    "                test_writer = tf.summary.FileWriter(TensorBoard_Folder_test)\n",
    "\n",
    "                # EVALUATION\n",
    "                print(\"eval\")\n",
    "                # pretty much the same thing but no train step.\n",
    "                # print(\"eval\")\n",
    "                # TEST SET\n",
    "                startindex = cutoff + offset # start at the first item in the last 10%\n",
    "                endindex = startindex + batchsize\n",
    "                xe_sum = 0\n",
    "                print(startindex)\n",
    "                numbatches = 0\n",
    "                while (endindex <= cutoff + 200 * batchsize + offset):  # trainsize):\n",
    "                    numbatches = numbatches + 1  # TODO: just calculate this\n",
    "                    # print(startindex)\n",
    "                    if(numbatches%10 == 0):\n",
    "                        print(numbatches)\n",
    "                    if TASK == \"PREDICT_LABELS\":\n",
    "                        xe = sess.run(nets['cross_entroy'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabels[startindex:endindex], nets['keep_prob']: 1})\n",
    "                    else:\n",
    "                        xe = sess.run(nets['cross_entroy'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabels[startindex:endindex], nets['keep_prob']: 1})\n",
    "\n",
    "                    xe_sum =  xe_sum+ xe\n",
    "                    startindex = endindex\n",
    "                    endindex = startindex + batchsize\n",
    "\n",
    "                # print(nets['global_step'].eval(session=sess))\n",
    "                print(\"xesum \",xe_sum )\n",
    "                print(\"nb \", numbatches)\n",
    "                print(\"thing \",xe_sum / numbatches )\n",
    "                xesummary = sess.run(nets['xesum'], feed_dict={nets['xe_ave']: xe_sum / numbatches})\n",
    "                test_writer.add_summary(xesummary, nets['global_step'].eval(session=sess))\n",
    "\n",
    "                # TRAIN\n",
    "                print(\"train\")\n",
    "                offset = 0#random.randint(0,cutoff - 200*100)\n",
    "                startindex = 0  + offset\n",
    "                endindex = startindex + batchsize\n",
    "                xe_sum = 0\n",
    "                numbatches = 0\n",
    "                # loop through the first 90% of the set     \n",
    "                while (endindex <= batchsize * 200 + offset):\n",
    "                    # Get the indeces as ints for the labels\n",
    "                    numbatches = numbatches + 1  # TODO: just calculate this\n",
    "                    if(numbatches%10 == 0):\n",
    "                        print(numbatches)\n",
    "                    if TASK == \"PREDICT_LABELS\":\n",
    "                        xe = sess.run(nets['cross_entroy'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabels[startindex:endindex], nets['keep_prob']: .5})\n",
    "                    else:\n",
    "                        xe = sess.run(nets['cross_entroy'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabels[startindex:endindex], nets['keep_prob']: .5})\n",
    "\n",
    "                    xe_sum =  xe_sum+ xe\n",
    "                    startindex = endindex\n",
    "                    endindex = startindex + batchsize\n",
    "\n",
    "                # print(nets['global_step'].eval(session=sess))\n",
    "                print(\"xesum \",xe_sum )\n",
    "                print(\"nb \", numbatches)\n",
    "                print(\"thing \",xe_sum / numbatches )\n",
    "                xesummary = sess.run(nets['xesumt'], feed_dict={nets['xe_avet']: xe_sum / numbatches})\n",
    "                train_writer.add_summary(xesummary, nets['global_step'].eval(session=sess))\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "# builds the components\n",
    "def conv2d(inputtensor, Weights, bias, strides=1, name=None, padding=\"SAME\"):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(inputtensor, Weights, strides=[1, strides, strides, 1], padding=padding, name=name)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2x2(x, k=2, name=None, padding=\"SAME\"):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding=padding, name=name)\n",
    "\n",
    "\n",
    "def add_hanning_pooling(nets, top_node, downsample=2, length_of_window=8, layer_name=False, make_plots=False):\n",
    "    \"\"\"\n",
    "    Add a layer using a hanning kernel for pooling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nets : dictionary\n",
    "        a dictionary containing the graph\n",
    "    downsample : int\n",
    "        proportion downsampling\n",
    "    top_node : string\n",
    "        specify the node after which the spectemp filters will be added and used as input for the FFT.\n",
    "    layer_name : False or string\n",
    "        name for the layer. If false appends \"_hpool\" to the top_node name\n",
    "    Returns\n",
    "    -------\n",
    "    nets : dictionary\n",
    "        a dictionary containing the graph, containing a pooling layer\n",
    "    \"\"\"\n",
    "    if not layer_name:\n",
    "        layer_name = top_node + '_hpool'\n",
    "\n",
    "    n_channels = nets[top_node].get_shape().as_list()[3]\n",
    "    nets['hanning_tensor_' + top_node] = make_hanning_kernel_tensor(n_channels, downsample=downsample,\n",
    "                                                                    length_of_window=length_of_window,\n",
    "                                                                    make_plots=make_plots)\n",
    "    return tf.nn.depthwise_conv2d(nets[top_node], nets['hanning_tensor_' + top_node], [1, downsample, downsample, 1],\n",
    "                                  'SAME', name=layer_name)\n",
    "\n",
    "\n",
    "def make_hanning_kernel_tensor(n_channels, downsample=2, length_of_window=8, make_plots=False):\n",
    "    \"\"\"\n",
    "    Make a tensor containing the symmetric 2d hanning kernel to use for the pooling filters\n",
    "    For downsample=2, using length_of_window=8 gives a reduction of -24.131545969216841 at 0.25 cycles\n",
    "    For downsample=3, using length_of_window=12 gives a reduction of -28.607805482176282 at 1/6 cycles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        number of channels to copy the kernel into\n",
    "    downsample : int\n",
    "        proportion downsampling\n",
    "    length_of_window : int\n",
    "        how large of a window to use\n",
    "    make_plots: boolean\n",
    "        make plots of the filters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hanning_tensor : tensorflow tensor\n",
    "        tensorflow tensor containing the hanning kernel with size [1 length_of_window length_of_window n_channels]\n",
    "\n",
    "    \"\"\"\n",
    "    hanning_kernel = make_hanning_kernel(downsample=downsample, length_of_window=length_of_window,\n",
    "                                         make_plots=make_plots)\n",
    "    hanning_kernel = np.expand_dims(np.dstack([hanning_kernel.astype(np.float32)] * n_channels), axis=3)\n",
    "    hanning_tensor = tf.constant(hanning_kernel)\n",
    "    return hanning_tensor\n",
    "\n",
    "\n",
    "def make_hanning_kernel(downsample=2, length_of_window=8, make_plots=False):\n",
    "    \"\"\"\n",
    "    Make the symmetric 2d hanning kernel to use for the pooling filters\n",
    "    For downsample=2, using length_of_window=8 gives a reduction of -24.131545969216841 at 0.25 cycles\n",
    "    For downsample=3, using length_of_window=12 gives a reduction of -28.607805482176282 at 1/6 cycles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    downsample : int\n",
    "        proportion downsampling\n",
    "    length_of_window : int\n",
    "        how large of a window to use\n",
    "    make_plots: boolean\n",
    "        make plots of the filters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    two_dimensional_kernel : numpy array\n",
    "        hanning kernel in 2d to use as a kernel for filtering\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    window = 0.5 * (1 - np.cos(2 * np.pi * (np.arange(length_of_window)) / (length_of_window - 1)))\n",
    "    A = np.fft.fft(window, 2048) / (len(window) / 2.0)\n",
    "    freq = np.linspace(-0.5, 0.5, len(A))\n",
    "    response = 20 * np.log10(np.abs(np.fft.fftshift(A / abs(A).max())))\n",
    "\n",
    "    nyquist = 1 / (2 * downsample)\n",
    "    ny_idx = np.where(np.abs(freq - nyquist) == np.abs(freq - nyquist).min())[0][0]\n",
    "    two_dimensional_kernel = np.sqrt(np.outer(window, window))\n",
    "\n",
    "    if make_plots:\n",
    "        print(['Frequency response at ' + 'nyquist' + ' is ' + 'response[ny_idx]'])\n",
    "        plt.figure()\n",
    "        plt.plot(window)\n",
    "        plt.title(r\"Hanning window\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.figure()\n",
    "        plt.plot(freq, response)\n",
    "        plt.axis([-0.5, 0.5, -120, 0])\n",
    "        plt.title(r\"Frequency response of the Hanning window\")\n",
    "        plt.ylabel(\"Normalized magnitude [dB]\")\n",
    "        plt.xlabel(\"Normalized frequency [cycles per sample]\")\n",
    "        plt.figure()\n",
    "        plt.matshow(two_dimensional_kernel)\n",
    "        plt.colorbar()\n",
    "        plt.title(r\"Two dimensional kernel\")\n",
    "\n",
    "    return two_dimensional_kernel\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    debug = True  # if true, then use the default csv file to read from.\n",
    "    if debug:\n",
    "        learning_rate = 1e-5\n",
    "        music_only = False  # currently also removing speech\n",
    "        limit = None\n",
    "        Net_or_VGG = \"Net\"\n",
    "        name = \"BBB_TEST5\"\n",
    "        unbalanced = True\n",
    "        folder = \"TB3\"\n",
    "        Conv1_filtersize = 9\n",
    "        TASK = \"PREDICT_LABELS\"\n",
    "        # TASK = \"PREDICT_COUNTS\"\n",
    "        OVERIDE_FOLDER = \"Net_UNBALANCED91_5_nfull_lr1e-05\"\n",
    "        #OVERIDE_FOLDER =  \"Net_UNBALANCED91_5_nfull_lr1e-05\"\n",
    "        SAVE = False\n",
    "        padding = \"SAME\"\n",
    "        # padding = \"VALID\"\n",
    "        # poolmethod =\"HPOOL\"\n",
    "        poolmethod = \"MAXPOOL\"\n",
    "        main(learning_rate, limit, music_only, Net_or_VGG=Net_or_VGG, name=name, unbalanced=unbalanced, folder=folder,\n",
    "             Conv1_filtersize=Conv1_filtersize, TASK=TASK, OVERIDE_FOLDER=OVERIDE_FOLDER, SAVE=SAVE, padding=padding,\n",
    "             poolmethod=poolmethod)\n",
    "\n",
    "    else:\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('-a', '--learning_rate', default=1e-6)\n",
    "        parser.add_argument('-b', '--limit', default=None)\n",
    "        parser.add_argument('-c', '--Model', default=\"Net\")\n",
    "        parser.add_argument('-n', '--name', default=\"noname\")\n",
    "        parser.add_argument('-f', '--conv1filtersize', default=9)\n",
    "        parser.add_argument('-t', '--TASK', default=\"PREDICT_LABELS\")\n",
    "        parser.add_argument('-o', '--OVERIDE_FOLDER', default=None)\n",
    "\n",
    "        args = vars(parser.parse_args())\n",
    "\n",
    "        learning_rate = float(args[\"learning_rate\"])\n",
    "\n",
    "        if (not args[\"limit\"] == None):\n",
    "            limit = int(args[\"limit\"])\n",
    "        else:\n",
    "            limit = None\n",
    "\n",
    "        Net_or_VGG = args[\"Model\"]\n",
    "        name = args['name']\n",
    "        Conv1_filtersize = int(args['conv1filtersize'])\n",
    "        TASK = args['TASK']\n",
    "\n",
    "        if (not args['OVERIDE_FOLDER'] == None):\n",
    "            OVERIDE_FOLDER = args['OVERIDE_FOLDER']\n",
    "        else:\n",
    "            OVERIDE_FOLDER = None\n",
    "\n",
    "        music_only = False\n",
    "        unbalanced = True\n",
    "        folder = \"TB3\"\n",
    "        poolmethod = \"MAXPOOL\"\n",
    "        padding = \"SAME\"\n",
    "        SAVE = False\n",
    "\n",
    "        main(learning_rate, limit, music_only, Net_or_VGG=Net_or_VGG, name=name, unbalanced=unbalanced, folder=folder,Conv1_filtersize=Conv1_filtersize, TASK=TASK, OVERIDE_FOLDER=OVERIDE_FOLDER, SAVE=SAVE, padding=padding,poolmethod=poolmethod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
