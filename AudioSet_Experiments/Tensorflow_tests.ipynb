{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "#thanks to https://gist.githubusercontent.com/kukuruza/03731dc494603ceab0c5/raw/3d708320145df0a962cfadb95b3f716b623994e0/gist_cifar10_train.py\n",
    "def put_kernels_on_grid (kernel, grid_Y, grid_X, pad = 1):\n",
    "\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x3 = tf.reshape(x2, tf.stack([grid_X, Y * grid_Y, X, channels])) #3\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x5 = tf.reshape(x4, tf.stack([1, X * grid_X, Y * grid_Y, channels])) #3\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scale to [0, 255] and convert to uint8\n",
    "    return tf.image.convert_image_dtype(x7, dtype = tf.uint8)\n",
    "\n",
    "def main():\n",
    "    #sess = tf.InteractiveSession()\n",
    "    \n",
    "    trainfile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_Coch.hdf5\"\n",
    "    train_averagefile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_average.npy\"\n",
    "    testfile =  trainfile#\"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_Coch.hdf5\"\n",
    "    test_averagefile = train_averagefile #\"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_average.npy\"\n",
    "    trainset = h5py.File(trainfile)\n",
    "    testset = h5py.File(testfile)\n",
    "    \n",
    "    #TODO: Subtract this when doing the model\n",
    "    train_ave_coch = np.load(train_averagefile)\n",
    "    test_ave_coch = np.load(test_averagefile)\n",
    "    \n",
    "    COCHLEAGRAM_LENGTH = int(342000/1) # full is 342000\n",
    "    conv1_strides = 3\n",
    "    conv2_strides = 2\n",
    "    conv3_strides = 1\n",
    "    conv4_strides = 1\n",
    "    conv5_strides = 1\n",
    "    \n",
    "    trainsize = trainset.attrs.get(\"size\")\n",
    "    testsize = testset.attrs.get(\"size\")\n",
    "    \n",
    "    b = 0\n",
    "    batchsize = 1\n",
    "    \n",
    "    final_filter = 512\n",
    "    full_length = final_filter * math.ceil(COCHLEAGRAM_LENGTH/171/conv1_strides/conv2_strides/conv3_strides/conv4_strides/conv5_strides/4)* math.ceil(171/conv1_strides/conv2_strides/conv3_strides/conv4_strides/conv5_strides/4)\n",
    "    \n",
    "    nets = {}\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            nets['input_to_net'] = tf.placeholder(tf.float32, [None,COCHLEAGRAM_LENGTH], name='input_to_net')\n",
    "            nets['actual_labels'] = tf.placeholder(tf.float32, [None, 527], name='actual_labels')\n",
    "            nets['label_indeces'] = tf.placeholder(tf.int32, [None, 15], name='label_indeces')\n",
    "            \n",
    "            \n",
    "            nets['accuracy'] = tf.placeholder(tf.float32, (), name='acc')\n",
    "            nets['accsum'] = tf.summary.scalar(\"accuracy\",nets['accuracy'])\n",
    "            \n",
    "            \n",
    "            nets['reshapedCoch'] = tf.reshape(nets['input_to_net'],[-1,171,int(COCHLEAGRAM_LENGTH/171),1], name='reshape_input')\n",
    "\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        #thing = sess.run(reshapedCoch,feed_dict={input_to_net:trainset[\"/coch\"][0:2],actual_labels:trainset[\"/labels\"][0:2]})\n",
    "\n",
    "\n",
    "\n",
    "        with tf.variable_scope('image_filters') as scope:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            conv1_Weights = weight_variable([9,9,1,96])\n",
    "            \n",
    "            grid = put_kernels_on_grid(conv1_Weights,16,6)\n",
    "            nets['conv1_weight_image'] = tf.summary.image('conv1/kernels', grid, max_outputs=3)\n",
    "            \n",
    "            nets['conv1'] = conv2d(nets['reshapedCoch'], Weights = conv1_Weights, bias = bias_variable([96]), strides=conv1_strides, name='conv1')\n",
    "            nets['maxpool1'] = maxpool2x2(nets['conv1'],k=2 , name='maxpool1')\n",
    "\n",
    "\n",
    "            \n",
    "            #nets['conv1_image'] = tf.reshape(nets['conv1'],[])\n",
    "\n",
    "        with tf.name_scope('conv2'):\n",
    "            nets['conv2'] = conv2d(nets['maxpool1'], Weights = weight_variable([5,5,96,256]), bias = bias_variable([256]), strides=conv2_strides, name='conv2')\n",
    "            nets['maxpool2'] = maxpool2x2(nets['conv2'],k=2 , name='maxpool2')\n",
    "\n",
    "        with tf.name_scope('conv3'):\n",
    "            nets['conv3'] = conv2d(nets['maxpool2'], Weights = weight_variable([3,3,256,512]), bias = bias_variable([512]), strides=conv3_strides, name='conv3')\n",
    "\n",
    "        with tf.name_scope('conv4'):\n",
    "            nets['conv4'] = conv2d(nets['conv3'], Weights = weight_variable([3,3,512,1024]), bias = bias_variable([1024]), strides=conv4_strides, name='conv4')        \n",
    "\n",
    "        with tf.name_scope('conv5'):\n",
    "            nets['conv5'] = conv2d(nets['conv4'], Weights = weight_variable([3,3,1024,final_filter]), bias = bias_variable([final_filter]),strides=conv5_strides, name='conv5')             \n",
    "            nets['flattened'] = tf.reshape(nets['conv5'], [-1, full_length])\n",
    "\n",
    "        with tf.name_scope('fc_1'):\n",
    "            W_fc1 = weight_variable([full_length, 1024]) # 4,959,232\n",
    "            b_fc1 = bias_variable([1024])\n",
    "            nets['fc_1'] = tf.nn.relu(tf.matmul(nets['flattened'], W_fc1) + b_fc1)\n",
    "\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            nets['h_fc1_drop'] = tf.nn.dropout(nets['fc_1'], keep_prob)\n",
    "\n",
    "        with tf.name_scope('fc_2'):\n",
    "            W_fc2 = weight_variable([1024, 527])\n",
    "            b_fc2 = bias_variable([527])\n",
    "        \n",
    "        with tf.name_scope('predictions'):\n",
    "            nets['predicted_labels'] = tf.matmul(nets['h_fc1_drop'], W_fc2) + b_fc2\n",
    "\n",
    "        with tf.variable_scope(\"eval\"):\n",
    "            nets['cross_entroy'] = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels =nets['actual_labels'],logits = nets['predicted_labels'] ))\n",
    "            nets['train_step'] = tf.train.AdamOptimizer(1e-4).minimize(nets['cross_entroy'])\n",
    "\n",
    "            #num = np.sum(nets['actual_labelsi'])\n",
    "            #nets['accuracy'] = tf.metrics.mean(tf.nn.in_top_k(nets['predicted_labels'],nets['actual_labelsi'],2))\n",
    "\n",
    "            #calculating accuracys\n",
    "\n",
    "            _,nets['indeces'] = tf.nn.top_k(nets['predicted_labels'],15)\n",
    "            nets['numCorrect'] = tf.shape(tf.sets.set_intersection(nets['indeces'],nets['label_indeces'],False).values)[0]\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer = tf.summary.FileWriter('./TB/train5', sess.graph)\n",
    "            for i in range(sys.maxsize):\n",
    "                if(i%10 == 0 ):\n",
    "                    print(i)                \n",
    "                #get random batch\n",
    "                start = random.randint(0,trainsize-batchsize)\n",
    "                end = start+batchsize     \n",
    "                thing = sess.run(nets['train_step'],feed_dict={nets['input_to_net']:trainset[\"/coch\"][start:end][:,0:COCHLEAGRAM_LENGTH]-train_ave_coch[0:COCHLEAGRAM_LENGTH],nets['actual_labels']:trainset[\"/labels\"][start:end],keep_prob:.5})\n",
    "                \n",
    "                \n",
    "                if(i%200 == 0):\n",
    "                    \n",
    "                    total_count = 0\n",
    "                    correct = 0\n",
    "                    \n",
    "                    for x in range(100):\n",
    "                        #get random batch\n",
    "                        start = random.randint(0,testsize-batchsize)\n",
    "                        end = start+batchsize\n",
    "                        trainBatchLabels = testset[\"/labels\"][start:end]\n",
    "    \n",
    "                        indeces_list = []\n",
    "\n",
    "                        for row in trainBatchLabels:\n",
    "                            indeces= np.where(row==1)[0]\n",
    "                            total_count = total_count + len(indeces)\n",
    "                            indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "                        \n",
    "    \n",
    "                        correct = correct + sess.run(nets['numCorrect'],feed_dict={nets['input_to_net']:testset[\"/coch\"][start:end][:,0:COCHLEAGRAM_LENGTH]-test_ave_coch[0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testset[\"/labels\"][start:end],nets['label_indeces']:indeces_list,keep_prob:1})\n",
    "                        potato = sess.run(nets['indeces'],feed_dict={nets['input_to_net']:testset[\"/coch\"][start:end][:,0:COCHLEAGRAM_LENGTH]-test_ave_coch[0:COCHLEAGRAM_LENGTH],nets['actual_labels']:testset[\"/labels\"][start:end],nets['label_indeces']:indeces_list,keep_prob:1})\n",
    "\n",
    "                    summary = sess.run(nets['accsum'],feed_dict={nets['accuracy']:correct/total_count})\n",
    "                    train_writer.add_summary(summary, i)\n",
    "                    \n",
    "                    image_conv1 = sess.run(nets['conv1_weight_image'])\n",
    "                    train_writer.add_summary(image_conv1, i)\n",
    "                    \n",
    "                    print()\n",
    "                    print(\"loop {0}, accuracy {1}\".format(i,correct/total_count))\n",
    "                    #print(start)\n",
    "                    #print(end)\n",
    "                    #print(total_count)\n",
    "                    #print(correct)\n",
    "                    #print(\"actual: \", indeces_list)\n",
    "                    #print(\"predicted :\", potato)\n",
    "                    #print()\n",
    "                    #f= open(\"output3.csv\",\"a\")\n",
    "                    #f.write(\"loop {0}, accuracy {1}\\n\".format(i,correct/total_count))\n",
    "                    #f.close()\n",
    "    \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#builds the components\n",
    "def conv2d(inputtensor, Weights, bias, strides=1,name = None):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(inputtensor, Weights, strides=[1, strides, strides, 1], padding='SAME',name=name)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2x2(x, k=2, name = None):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME',name=name)\n",
    "    \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "out = tf.Variable( [[0,50,0,99], [0,50,1,0],[999,50,0,99]])\n",
    "\n",
    "actual = tf.Variable([[0,1,0,1], [0,1,1,0],[1,0,0,0]])\n",
    "\n",
    "condition = tf.equal(actual,1)\n",
    "\n",
    "\n",
    "_,indeces = tf.nn.top_k(out,2)\n",
    "\n",
    "preds = tf.Variable([[8,3], [1,2],[2,3]])\n",
    "targ = tf.Variable([[8,3], [2,1],[2,3]])\n",
    "\n",
    "results = tf.shape(tf.sets.set_intersection(indeces,targ,False).values)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"indeces \",sess.run(indeces))\n",
    "    print(\"results\", sess.run(results))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[0,1,0,1], [0,1,1,0],[1,0,0,0]])\n",
    "indeces = []\n",
    "for row in a:\n",
    "    indeces.append(np.where(row == 1)[0])\n",
    "\n",
    "print(indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "trainfile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_Coch.hdf5\"\n",
    "train_averagefile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_average.npy\"\n",
    "\n",
    "trainset = h5py.File(trainfile)\n",
    "start = 0\n",
    "end = 5\n",
    "trainBatchLabels = trainset[\"/labels\"][start:end]\n",
    "\n",
    "indeces_list = []\n",
    "for row in trainBatchLabels:\n",
    "    indeces= np.where(row==1)[0]\n",
    "    print(indeces)\n",
    "    indeces_list.append(np.pad(indeces,[0,15-len(indeces)],mode = 'constant',constant_values=-1))\n",
    "\n",
    "print()\n",
    "print(len(indeces_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing reshape for image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def put_kernels_on_grid (kernel, pad = 1):\n",
    "\n",
    "    '''Visualize conv. filters as an image (mostly for the 1st layer).\n",
    "    Arranges filters into a grid, with some paddings between adjacent filters.\n",
    "\n",
    "    Args:\n",
    "    kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "    pad:               number of black pixels around each filter (between them)\n",
    "\n",
    "    Return:\n",
    "    Tensor of shape [1, (Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels].\n",
    "    '''\n",
    "    # get shape of the grid. NumKernels == grid_Y * grid_X\n",
    "    def factorization(n):\n",
    "        for i in range(int(sqrt(float(n))), 0, -1):\n",
    "            if n % i == 0:\n",
    "                if i == 1: print('Who would enter a prime number of filters')\n",
    "                return (i, int(n / i))\n",
    "    (grid_Y, grid_X) = factorization (kernel.get_shape()[3].value)\n",
    "    print ('grid: %d = (%d, %d)' % (kernel.get_shape()[3].value, grid_Y, grid_X))\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "    kernel = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x = tf.pad(kernel, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel.get_shape()[0] + 2 * pad\n",
    "    X = kernel.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x = tf.transpose(x, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x = tf.reshape(x, tf.stack([grid_X, Y * grid_Y, X, channels]))\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x = tf.transpose(x, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x = tf.reshape(x, tf.stack([1, X * grid_X, Y * grid_Y, channels]))\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x = tf.transpose(x, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x = tf.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "    # scaling to [0, 255] is not necessary for tensorboard\n",
    "    return x\n",
    "\n",
    "\n",
    "#\n",
    "# ... and somewhere inside \"def train():\" after calling \"inference()\"\n",
    "#\n",
    "\n",
    "# Visualize conv1 kernels\n",
    "#with tf.variable_scope('conv1'):\n",
    "#  tf.get_variable_scope().reuse_variables()\n",
    "#  weights = tf.get_variable('weights')\n",
    "#  grid = put_kernels_on_grid (weights)\n",
    "#  tf.image.summary('conv1/kernels', grid, max_outputs=1)\n",
    "\n",
    "\n",
    "\n",
    "a = np.arange(81).reshape((3,3,1,9))\n",
    "\n",
    "print(a)\n",
    "\n",
    "first = tf.placeholder(tf.int32,[3,3,1,9], name = 'startShape')\n",
    "second = put_kernels_on_grid(first, pad = 1)\n",
    "#second = tf.reshape(first,[9,9])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(second, feed_dict = {first:a}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
