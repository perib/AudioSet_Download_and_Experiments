{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO SAVE PER EPOCH IN NON UNBALANCED SET\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "from math import sqrt\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from PedrosNetworkFunctionsALPHA import *\n",
    "\n",
    "\n",
    "def main(Learning_Rate, limit=None, music_only=False, print_extras=True, Net_or_VGG=\"Net\", name=\"\", unbalanced=False,\n",
    "         folder=\"TB2\", Conv1_filtersize=9, TASK=\"PREDICT_LABELS\", OVERIDE_FOLDER=None, SAVE=True, padding=\"SAME\",\n",
    "         poolmethod=\"MAXPOOL\", conv1_times_hanning=False):\n",
    "    # if we want to use the unbalanced set for training/eval, read that file in\n",
    "    if (unbalanced):\n",
    "        trainfile = \"/om/user/ribeirop/audiosetDL/unbalanced_stripped/unbalanced_train_segments_Coch.hdf5\"\n",
    "        train_averagefile = \"/om/user/ribeirop/audiosetDL/unbalanced_stripped/unbalanced_train_segments_average.npy\"\n",
    "    else:\n",
    "        trainfile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_Coch.hdf5\"\n",
    "        train_averagefile = \"/om/user/ribeirop/audiosetDL/balanced_stripped/balanced_train_segments_average.npy\"\n",
    "\n",
    "    testfile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_Coch.hdf5\"\n",
    "    test_averagefile = \"/om/user/ribeirop/audiosetDL/eval_stripped/eval_segments_average.npy\"\n",
    "    trainset = h5py.File(trainfile, \"r\")\n",
    "    testset = h5py.File(testfile, \"r\")\n",
    "\n",
    "    train_coch = trainset[\"/coch\"]  # The cochleagrams are in here\n",
    "    test_coch = testset[\"/coch\"]\n",
    "\n",
    "    #    unbalanced_label_counts = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_label_counts.npy\")\n",
    "\n",
    "\n",
    "    if (TASK == \"PREDICT_LABELS\"):\n",
    "        multiple_labels = True\n",
    "        if (music_only):  # If we want to use a label set without music\n",
    "            if (unbalanced):\n",
    "                trainlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_train_segments_nomusic_labels_only.hdf5\"\n",
    "            else:\n",
    "                trainlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/balanced_train_segments_nomusic_labels_only.hdf5\"\n",
    "\n",
    "            testlabels_file = \"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/eval_segments_nomusic_labels_only.hdf5\"\n",
    "\n",
    "            trainlabelsSet = h5py.File(trainlabels_file, \"r\")\n",
    "            testlabelsSet = h5py.File(testlabels_file, \"r\")\n",
    "\n",
    "            trainlabels = trainlabelsSet[\"/labels\"]\n",
    "            testlabels = testlabelsSet[\"/labels\"]\n",
    "\n",
    "            numlabels = 525\n",
    "        else:\n",
    "            trainlabels = trainset[\"/labels\"]\n",
    "            testlabels = testset[\"/labels\"]\n",
    "\n",
    "            numlabels = 527\n",
    "    elif (TASK == \"PREDICT_COUNTS\"):\n",
    "        multiple_labels = False\n",
    "        if (unbalanced):\n",
    "            trainlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/unbalanced_label_counts.npy\")\n",
    "        else:\n",
    "            trainlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/balanced_label_counts.npy\")\n",
    "\n",
    "        testlabels = np.load(\"/home/ribeirop/OMFOLDER/audiosetDL/MODIFIED_LABELS/eval_label_counts.npy\")\n",
    "        numlabels = 15\n",
    "\n",
    "    COCHLEAGRAM_LENGTH = int(342000)  # full is 342000\n",
    "    train_mean_coch = np.load(train_averagefile)[0:COCHLEAGRAM_LENGTH]  # mean cochleagram\n",
    "    # test_mean_coch = np.load(test_averagefile)\n",
    "\n",
    "    Num_Epochs = 2000\n",
    "\n",
    "    if (not limit == None):\n",
    "        trainsize = limit\n",
    "        testsize = limit\n",
    "    else:\n",
    "        trainsize = trainset.attrs.get(\"size\")\n",
    "        testsize = testset.attrs.get(\"size\")\n",
    "        limit = \"full\"\n",
    "\n",
    "    # for unbalanced set\n",
    "    cutoff = int(trainsize * .90)\n",
    "\n",
    "    b = 0\n",
    "    batchsize = 100\n",
    "\n",
    "    for epoch_step in range(0, 50):\n",
    "        epoch_step_folder = \"EPOCH_SAVED/{0}/model.ckpt-{1}\".format(OVERIDE_FOLDER, epoch_step)\n",
    "\n",
    "        print(epoch_step_folder)\n",
    "\n",
    "        print(\"building graph\")\n",
    "        # The training of the graphs are defined here.\n",
    "        with tf.Graph().as_default():\n",
    "\n",
    "            if OVERIDE_FOLDER == None:\n",
    "                # Get the base graph\n",
    "                if (Net_or_VGG == \"Net\"):\n",
    "                    TensorBoard_Folder_train = './{4}/{0}_{6}_{1}_n{2}_lr{3}_conv1FS{5}_TRAIN'.format(Net_or_VGG, name,\n",
    "                                                                                                      limit,\n",
    "                                                                                                      Learning_Rate,\n",
    "                                                                                                      folder,\n",
    "                                                                                                      Conv1_filtersize,\n",
    "                                                                                                      TASK)\n",
    "                    TensorBoard_Folder_test = './{4}/{0}_{6}_{1}_n{2}_lr{3}_conv1FS{5}_TEST'.format(Net_or_VGG, name,\n",
    "                                                                                                    limit,\n",
    "                                                                                                    Learning_Rate,\n",
    "                                                                                                    folder,\n",
    "                                                                                                    Conv1_filtersize,\n",
    "                                                                                                    TASK)\n",
    "                    Saver_Folder = '/{0}_{5}_{1}_n{2}_lr{3}_conv1FS{4}'.format(Net_or_VGG, name, limit, Learning_Rate,\n",
    "                                                                               Conv1_filtersize, TASK)\n",
    "                    nets = Gen_audiosetNet(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch, Conv1_filtersize, padding,\n",
    "                                           poolmethod, conv1_times_hanning)\n",
    "                else:\n",
    "                    TensorBoard_Folder_train = './{4}/{0}_{4}_{1}_n{2}_lr{3}_TRAIN'.format(Net_or_VGG, name, limit,\n",
    "                                                                                           Learning_Rate, folder, TASK)\n",
    "                    TensorBoard_Folder_test = './{4}/{0}_{4}_{1}_n{2}_lr{3}_TEST'.format(Net_or_VGG, name, limit,\n",
    "                                                                                         Learning_Rate, folder, TASK)\n",
    "                    Saver_Folder = '/{0}_{4}_{1}_n{2}_lr{3}'.format(Net_or_VGG, name, limit, Learning_Rate, TASK)\n",
    "                    nets = Gen_VGG(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch)\n",
    "            else:\n",
    "                TensorBoard_Folder_train = './{0}/{1}_TRAIN'.format(folder, OVERIDE_FOLDER)\n",
    "                TensorBoard_Folder_test = './{0}/{1}_TEST'.format(folder, OVERIDE_FOLDER)\n",
    "                Saver_Folder = '/{0}'.format(OVERIDE_FOLDER)\n",
    "                if (Net_or_VGG == \"Net\"):\n",
    "                    nets = Gen_audiosetNet(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch, Conv1_filtersize, padding,\n",
    "                                           poolmethod, conv1_times_hanning)\n",
    "                else:\n",
    "                    nets = Gen_VGG(COCHLEAGRAM_LENGTH, numlabels, train_mean_coch)\n",
    "\n",
    "            print(TensorBoard_Folder_train)\n",
    "            print(TensorBoard_Folder_test)\n",
    "            # Get the loss functions\n",
    "            Cross_Entropy_Train_on_Labels(nets, numlabels, Learning_Rate, multiple_labels)\n",
    "\n",
    "            merged = tf.summary.merge_all()\n",
    "\n",
    "            # CALC MISSING CROSS ENTROPY\n",
    "\n",
    "\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            with tf.Session() as sess:\n",
    "                \n",
    "                saver.restore(sess, epoch_step_folder)\n",
    "                #sess.run(tf.global_variables_initializer())\n",
    "                #sess.run(tf.initialize_all_variables())\n",
    "                #sess.run(tf.initialize_local_variables()) \n",
    "                offset = 0  # random.randint(0,100*2000)\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    TensorBoard_Folder_train)  # tensorboard writters for both train and test accuracy\n",
    "                test_writer = tf.summary.FileWriter(TensorBoard_Folder_test)\n",
    "\n",
    "                # EVALUATION\n",
    "                print(\"eval\")\n",
    "                # pretty much the same thing but no train step.\n",
    "                # print(\"eval\")\n",
    "                # TEST SET\n",
    "                startindex = cutoff + offset  # start at the first item in the last 10%\n",
    "                endindex = startindex + batchsize\n",
    "                xe_sum = 0\n",
    "                print(startindex)\n",
    "                numbatches = 0\n",
    "                calculator = AveragePrecisionCalculator()\n",
    "                MAPsum = 0\n",
    "                \n",
    "                sess.run(tf.initialize_local_variables())\n",
    "                for v in tf.local_variables():\n",
    "                    print(v)\n",
    "                \n",
    "                #varlist = ['AUC/auc_accumulator/true_positives:0','AUC/auc_accumulator/false_negatives:0','AUC/auc_accumulator/true_negatives:0','AUC/auc_accumulator/false_positives:0']     \n",
    "                #sess.run(tf.variables_initializer('AUC/auc_accumulator/true_positives:0') )              \n",
    "                \n",
    "                print(\"init auc \", sess.run(nets[\"auc\"]))\n",
    "                while (endindex <= cutoff + 100 * batchsize + offset):  # trainsize):\n",
    "                    numbatches = numbatches + 1  # TODO: just calculate this\n",
    "                    # print(startindex)\n",
    "                    if (numbatches % 10 == 0):\n",
    "                        print(numbatches)\n",
    "\n",
    "                    trainlabelbatch = trainlabels[startindex:endindex]\n",
    "                    if TASK == \"PREDICT_LABELS\":\n",
    "                        predictions = sess.run(nets['predicted_labels'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabelbatch, nets['keep_prob']: 1})\n",
    "                    else:\n",
    "                        predictions = sess.run(nets['predicted_labels'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabelbatch, nets['keep_prob']: 1})\n",
    "\n",
    "                    for i in range(len(predictions)):\n",
    "                        # print(predictions[i])\n",
    "                        # print(trainlabelbatch[i])\n",
    "                        normed = predictions[i]  # / abs(predictions[i]).max()\n",
    "                        sort = normed.argsort()[::-1]\n",
    "                        p = normed[sort][0:15]\n",
    "                        a=  trainlabelbatch[i][sort][0:15]\n",
    "                        c = len(np.where(trainlabelbatch[i] > 0)[0])\n",
    "                        calculator.accumulate(p,a,c)\n",
    "                        MAPsum = MAPsum + AveragePrecisionCalculator.ap_at_n(p,a,15,c)\n",
    "                        # calculator.accumulate(np.random.rand(527),trainlabelbatch[i])\n",
    "                        \n",
    "                    aucp = (predictions- predictions.min(axis=0))/(predictions.max(axis=0)-predictions.min(axis=0))\n",
    "                    sess.run(nets['update_auc'],feed_dict={nets['l']:trainlabelbatch,nets['p']:aucp})\n",
    "    \n",
    "                        \n",
    "                    startindex = endindex\n",
    "                    endindex = startindex + batchsize\n",
    "\n",
    "                # print(nets['global_step'].eval(session=sess))\n",
    "                print(\"MAP \", MAPsum / numbatches)\n",
    "                MAPsummary = sess.run(nets['MAPsum'], feed_dict={nets['MAP']: MAPsum / numbatches})\n",
    "                test_writer.add_summary(MAPsummary, nets['global_step'].eval(session=sess))\n",
    "\n",
    "                \n",
    "                print(\"GAP \",calculator.peek_ap_at_n())\n",
    "                GAPsummary = sess.run(nets['GAPsum'], feed_dict={nets['GAP']: calculator.peek_ap_at_n()})\n",
    "                test_writer.add_summary(GAPsummary, nets['global_step'].eval(session=sess))\n",
    "                \n",
    "                print(\"AUC \" ,sess.run(nets[\"auc\"]))\n",
    "                \n",
    "                test_writer.add_summary(sess.run(nets['AUCsum']), nets['global_step'].eval(session=sess))\n",
    "                \n",
    "\n",
    "                # TRAIN\n",
    "                print(\"train\")\n",
    "                offset = 0  # random.randint(0,cutoff - 200*100)\n",
    "                startindex = 0 + offset\n",
    "                endindex = startindex + batchsize\n",
    "                \n",
    "                \n",
    "                xe_sum = 0\n",
    "                \n",
    "                numbatches = 0\n",
    "                calculator = AveragePrecisionCalculator()\n",
    "                MAPsum = 0 #sum of the individuals APs\n",
    "                sess.run(tf.initialize_local_variables())\n",
    "                for v in tf.local_variables():\n",
    "                    print(v)\n",
    "                    \n",
    "                print(\"init auc \", sess.run(nets[\"auc\"]))\n",
    "                # loop through the first 90% of the set\n",
    "                while (endindex <= batchsize * 100 + offset):\n",
    "                    # Get the indeces as ints for the labels\n",
    "                    numbatches = numbatches + 1  # TODO: just calculate this\n",
    "                    trainlabelbatch = trainlabels[startindex:endindex]\n",
    "                    if (numbatches % 10 == 0):\n",
    "                        print(numbatches)\n",
    "                    if TASK == \"PREDICT_LABELS\":\n",
    "                        predictions = sess.run(nets['predicted_labels'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabelbatch, nets['keep_prob']: 1})\n",
    "                    else:\n",
    "                        predictions = sess.run(nets['predicted_labels'], feed_dict={\n",
    "                            nets['input_to_net']: train_coch[startindex:endindex][:, 0:COCHLEAGRAM_LENGTH],\n",
    "                            nets['actual_labels']: trainlabelbatch, nets['keep_prob']: 1})\n",
    "\n",
    "                        #predictions is the output of the last layer, that a list of activations per 527 labels\n",
    "                    \n",
    "                    for i in range(len(predictions)): #loop through the predictions for each sample in the batch\n",
    "                        normed = predictions[i]  # / abs(predictions[i]).max()\n",
    "                        sort = normed.argsort()[::-1] #sort from greatest to smallest\n",
    "                        p = normed[sort][0:15] #take the 15 biggest\n",
    "                        a=  trainlabelbatch[i][sort][0:15] #the label corresponding to the top 15 predictions (1 if present, 0 otherwise.)\n",
    "                        c = len(np.where(trainlabelbatch[i] > 0)[0]) #how many labels exist total.\n",
    "                        calculator.accumulate(p,a,c) #calculates GAP automatically\n",
    "                        MAPsum = MAPsum + AveragePrecisionCalculator.ap_at_n(p,a,15,c) #calculates precision of the point and adds it in.\n",
    "                    \n",
    "                    aucp = (predictions- predictions.min(axis=0))/(predictions.max(axis=0)-predictions.min(axis=0)) #normalize values to between 0 and 1 to calculate AUC\n",
    "                    sess.run(nets['update_auc'],feed_dict={nets['l']:trainlabelbatch,nets['p']:aucp}) #calculates AUC with tensorflows function.\n",
    "        \n",
    "                        \n",
    "                    startindex = endindex\n",
    "                    endindex = startindex + batchsize\n",
    "\n",
    "                #gets final values and saves\n",
    "                print(calculator.peek_ap_at_n())\n",
    "                GAPsummary = sess.run(nets['GAPsum'], feed_dict={nets['GAP']: calculator.peek_ap_at_n()})\n",
    "                train_writer.add_summary(GAPsummary, nets['global_step'].eval(session=sess))\n",
    "                \n",
    "                print(MAPsum / numbatches)\n",
    "                MAPsummary = sess.run(nets['MAPsum'], feed_dict={nets['MAP']: MAPsum / numbatches*100})\n",
    "                train_writer.add_summary(MAPsummary, nets['global_step'].eval(session=sess))\n",
    "                \n",
    "                print(\"AUC \" ,sess.run(nets[\"auc\"]))\n",
    "                train_writer.add_summary(sess.run(nets['AUCsum']), nets['global_step'].eval(session=sess))\n",
    "                \n",
    "               \n",
    "                \n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    debug = True  # if true, then use the default csv file to read from.\n",
    "    if debug:\n",
    "        learning_rate = 1e-4\n",
    "        music_only = False  # currently also removing speech\n",
    "        limit = None\n",
    "        Net_or_VGG = \"Net\"\n",
    "        name = \"BBB_TEST5\"\n",
    "        unbalanced = True\n",
    "        folder = \"TB_GMAP\"\n",
    "        Conv1_filtersize = 9\n",
    "        TASK = \"PREDICT_LABELS\"\n",
    "        # TASK = \"PREDICT_COUNTS\"\n",
    "        OVERIDE_FOLDER = \"Net_PREDICT_LABELS_A2UN91_4_HP9_nfull_lr0.0001_conv1FS9\"\n",
    "        # OVERIDE_FOLDER =  \"Net_UNBALANCED91_5_nfull_lr1e-05\"\n",
    "        SAVE = False\n",
    "        padding = \"SAME\"\n",
    "        # padding = \"VALID\"\n",
    "        poolmethod = \"HPOOL\"\n",
    "        # poolmethod = \"MAXPOOL\"\n",
    "        conv1_times_hanning = False\n",
    "        main(learning_rate, limit, music_only, Net_or_VGG=Net_or_VGG, name=name, unbalanced=unbalanced, folder=folder,\n",
    "             Conv1_filtersize=Conv1_filtersize, TASK=TASK, OVERIDE_FOLDER=OVERIDE_FOLDER, SAVE=SAVE, padding=padding,\n",
    "             poolmethod=poolmethod, conv1_times_hanning=conv1_times_hanning)\n",
    "\n",
    "    else:\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('-l', '--learning_rate' , default = 1e-5)\n",
    "        parser.add_argument('-z', '--limit', default = None)\n",
    "        parser.add_argument('-M', '--Model', default = \"Net\")\n",
    "        parser.add_argument('-n', '--name', default = \"noname\")\n",
    "        parser.add_argument('-f', '--conv1filtersize', default = 9)\n",
    "        parser.add_argument('-t', '--TASK',default = \"PREDICT_LABELS\")\n",
    "        parser.add_argument('-o', '--OVERIDE_FOLDER', default = None)\n",
    "        parser.add_argument('-p', '--poolmethod', default = \"MAXPOOL\")\n",
    "        parser.add_argument('-b', '--tbfolder', default = \"TB2\")\n",
    "        parser.add_argument('-m', '--musiconly', default = None)\n",
    "        parser.add_argument('-x', '--conv1_times_hanning', default = None)\n",
    "        parser.add_argument('-e', '--Freeze_Model_File', default = None)\n",
    "        parser.add_argument('-s', '--save', default = None)\n",
    "        \n",
    "        args = vars(parser.parse_args())\n",
    "\n",
    "        Freeze_Model_File = args[\"Freeze_Model_File\"]\n",
    "        \n",
    "        learning_rate = float(args[\"learning_rate\"])\n",
    "        \n",
    "        if (not args[\"limit\"] == None):\n",
    "            limit = int(args[\"limit\"])\n",
    "        else:\n",
    "            limit = None\n",
    "        \n",
    "        Net_or_VGG = args[\"Model\"]\n",
    "        name = args['name']\n",
    "        Conv1_filtersize = int(args['conv1filtersize'])\n",
    "        TASK = args['TASK']\n",
    "        \n",
    "        if (not args['OVERIDE_FOLDER'] == None):\n",
    "            OVERIDE_FOLDER = args['OVERIDE_FOLDER']\n",
    "        else:\n",
    "            OVERIDE_FOLDER = None\n",
    "        \n",
    "        if (not args['musiconly'] == None):\n",
    "            if (args['musiconly'] == \"True\"):\n",
    "                music_only = True\n",
    "            else:\n",
    "                music_only = False\n",
    "        else:\n",
    "            music_only = False\n",
    "        \n",
    "        \n",
    "        if (not args['conv1_times_hanning'] == None):\n",
    "            if (args['conv1_times_hanning'] == \"True\"):\n",
    "                conv1_times_hanning = True\n",
    "            else:\n",
    "                conv1_times_hanning = False\n",
    "        else:\n",
    "            conv1_times_hanning = False\n",
    "            \n",
    "            \n",
    "        if (not args['save'] == None):\n",
    "            if (args['save'] == \"True\"):\n",
    "                SAVE = True\n",
    "            else:\n",
    "                SAVE = False\n",
    "        else:\n",
    "            SAVE = True\n",
    "        \n",
    "        unbalanced = True\n",
    "        \n",
    "        folder =  args['tbfolder']\n",
    "        poolmethod =args['poolmethod']  \n",
    "        \n",
    "        padding = \"SAME\"\n",
    "        main(learning_rate, limit, music_only, Net_or_VGG=Net_or_VGG, name=name, unbalanced=unbalanced, folder=folder, Conv1_filtersize=Conv1_filtersize, TASK=TASK, OVERIDE_FOLDER=OVERIDE_FOLDER, SAVE=SAVE, padding=padding, poolmethod=poolmethod, conv1_times_hanning=conv1_times_hanning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
